{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Reviews Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bangwei Zhou (bz2280) Github: bzhouuu<br>\n",
    "James Jungsuk Lee (jl5241) Github: yjs1210<br>\n",
    "Ujjwal Peshin (up2138) Github: ujjwal95<br>\n",
    "Zhongling Jiang (zj2249) Github: jiangzl2016<br>\n",
    "\n",
    "## A. Abstract\n",
    "\n",
    "We compare the merits of n different recommendation algorithms side by side.\n",
    "\n",
    "* Biased Baseline\n",
    "* Collaborative Filtering (CF - Baseline)\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "* Hybrid Approach <br>\n",
    "\n",
    "Comparing the results of the models against our baseline shows that Collective Matrix Factorization Algorithm, DeepFM Algorithm, and Wide and Deep Algorithms outperform the baseline model. The strength of deep learning approaches were showcased as we increased the size of the dataset, beating the baselines initially by .02 RMSE in 20% subsampled data but later beating them by .04 RMSE in 100% of the data. Another key advantage of the deep learning approaches were the short training time. Using 100% of the dataset, they finished training in just 800 seconds on local environment while the CMF took an equivalent amount of time for dataset subsampled down to 50% size. Additionally, the weaknesses of the baseline models were exposed when metrics that measure user experiences such as ranking, coverage, and novelty were investigated. \n",
    "While our predictive algorithms didn’t provide drastic improvements in accuracy, our top deep learning methods providing about 3% improvement versus the baseline, given the fast training time as well as its ability to achieve superior measures on metrics that affect positive user experiences such as coverage, ranking, and novelty metrics, there is a strong reason to believe that deep learning models can provide value to Yelp users in a meaningful manner. In this report, we discuss the merits of various algorithms we explored and finally propose a complete recommendation system, that employs an ensemble of data retrieval system based on user location, cold-start recommendations using the bias-based model as well as DeepFM for general recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Business Case and Objective\n",
    "\n",
    "Our goal through this exercise is to figure out the best methodology for recommending restaurants to our users. This will be largely measured by using RMSE, which measures our forecast’s error relative to the ground truth as well as ranking metrics. Additionally, we want to measure metrics that can help us assess the sanity of our recommendation and experiences of the users. We measure these using coverage and novelty metrics to ensure that there is enough diversity in our recommendations and that our model isn’t resorting to recommending the most popular restaurants as opposed to creating a truly personalized experience. \n",
    " \n",
    "There are additional considerations such as model computation time, which affects long term overhead of maintenance of the product, and model comprehension. These are important considerations for the business as inefficient solutions will add technology debt and burden to the company’s systems. Also, employing models that are well understood can help the team in getting buy-in from stakeholders. Therefore, these factors will be critically analyzed when assessing the merits of our algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Data\n",
    "\n",
    "We use the publicly available yelp dataset which consists roughly of 7 million reviews from 1.6 million users across 190 thousand businesses in 10 metropolitan areas. We are additionally provided with business and user metadata as well as check-in information, tips and photos provided by the reviewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = tarfile.open('yelp_dataset.tar') \n",
    "zf.extract('review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6685900/6685900 [01:04<00:00, 104276.79it/s]\n"
     ]
    }
   ],
   "source": [
    "line_count = len(open(\"review.json\").readlines())\n",
    "user_ids, business_ids, stars, dates, texts = [], [], [], [], []\n",
    "with open(\"review.json\") as f:\n",
    "    for line in tqdm(f, total=line_count):\n",
    "        blob = json.loads(line)\n",
    "        user_ids += [blob[\"user_id\"]]\n",
    "        business_ids += [blob[\"business_id\"]]\n",
    "        stars += [blob[\"stars\"]]\n",
    "        dates += [blob[\"date\"]]\n",
    "        texts += [blob[\"text\"]]\n",
    "ratings_ = pd.DataFrame(\n",
    "    {\"user_id\": user_ids, \"business_id\": business_ids, \"rating\": stars, \"date\": dates, \"text\": texts}\n",
    ")\n",
    "user_counts = ratings_[\"user_id\"].value_counts()\n",
    "active_users = user_counts.loc[user_counts >= 5].index.tolist()\n",
    "ratings_ = ratings_.loc[ratings_.user_id.isin(active_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3fw2X5bZYeW9xCz_zGhOHg</td>\n",
       "      <td>2016-05-07 01:21:02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Tracy dessert had a big name in Hong Kong and ...</td>\n",
       "      <td>jlu4CztcSxrKx56ba1a5AQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zvO-PJCpNk4fgAVUnExYAA</td>\n",
       "      <td>2010-10-05 19:12:35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This place has gone down hill.  Clearly they h...</td>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b2jN2mm9Wf3RcrZCgfo1cg</td>\n",
       "      <td>2015-01-18 14:04:18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I was really looking forward to visiting after...</td>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg  2013-05-07 04:34:36     1.0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "6  3fw2X5bZYeW9xCz_zGhOHg  2016-05-07 01:21:02     3.0   \n",
       "7  zvO-PJCpNk4fgAVUnExYAA  2010-10-05 19:12:35     1.0   \n",
       "8  b2jN2mm9Wf3RcrZCgfo1cg  2015-01-18 14:04:18     2.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  Total bill for this horrible service? Over $8G...  hG7b0MtEbXx5QzbzE6C_VA  \n",
       "2  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  Tracy dessert had a big name in Hong Kong and ...  jlu4CztcSxrKx56ba1a5AQ  \n",
       "7  This place has gone down hill.  Clearly they h...  d6xvYpyzcfbF_AZ8vMB7QA  \n",
       "8  I was really looking forward to visiting after...  sG_h0dIzTKWa3Q6fmb4u-g  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 286130, unique restaurants: 185723\n"
     ]
    }
   ],
   "source": [
    "n_users = len(ratings_.user_id.unique())\n",
    "n_restaurants = len(ratings_.business_id.unique())\n",
    "print('Unique Users: {0}, unique restaurants: {1}'.format(n_users, n_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in User attributes set and Restaurant attributes set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ = pd.read_csv('active_users.csv')\n",
    "business_ = pd.read_csv('businesses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>user_yelping_since</th>\n",
       "      <th>friends</th>\n",
       "      <th>useful_reviews</th>\n",
       "      <th>funny_reviews</th>\n",
       "      <th>cool_reviews</th>\n",
       "      <th>n_fans</th>\n",
       "      <th>years_elite</th>\n",
       "      <th>average_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l6BmjZMeQD3rDxWUbiAiow</td>\n",
       "      <td>Rashmi</td>\n",
       "      <td>95</td>\n",
       "      <td>2013-10-08 23:11:33</td>\n",
       "      <td>c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2015,2016,2017</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4XChL029mKr5hydo79Ljxg</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>33</td>\n",
       "      <td>2013-02-21 22:29:06</td>\n",
       "      <td>kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bc8C_eETBWL0olvFSJJd0w</td>\n",
       "      <td>David</td>\n",
       "      <td>16</td>\n",
       "      <td>2013-10-04 00:16:10</td>\n",
       "      <td>4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MM4RJAeH6yuaN8oZDSt0RA</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>361</td>\n",
       "      <td>2013-10-23 07:02:50</td>\n",
       "      <td>mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...</td>\n",
       "      <td>1114</td>\n",
       "      <td>279</td>\n",
       "      <td>665</td>\n",
       "      <td>39</td>\n",
       "      <td>2015,2016,2017,2018</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEtzbpgA2BFBrC0y0sCbfw</td>\n",
       "      <td>Keane</td>\n",
       "      <td>1122</td>\n",
       "      <td>2006-02-15 18:29:35</td>\n",
       "      <td>RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...</td>\n",
       "      <td>13311</td>\n",
       "      <td>19356</td>\n",
       "      <td>15319</td>\n",
       "      <td>696</td>\n",
       "      <td>2006,2007,2008,2009,2010,2011,2012,2013</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id user_name  user_review_count   user_yelping_since  \\\n",
       "0  l6BmjZMeQD3rDxWUbiAiow    Rashmi                 95  2013-10-08 23:11:33   \n",
       "1  4XChL029mKr5hydo79Ljxg     Jenna                 33  2013-02-21 22:29:06   \n",
       "2  bc8C_eETBWL0olvFSJJd0w     David                 16  2013-10-04 00:16:10   \n",
       "3  MM4RJAeH6yuaN8oZDSt0RA     Nancy                361  2013-10-23 07:02:50   \n",
       "4  TEtzbpgA2BFBrC0y0sCbfw     Keane               1122  2006-02-15 18:29:35   \n",
       "\n",
       "                                             friends  useful_reviews  \\\n",
       "0  c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g...              84   \n",
       "1  kEBTgDvFX754S68FllfCaA, aB2DynOxNOJK9st2ZeGTPg...              48   \n",
       "2  4N-HU_T32hLENLntsNKNBg, pSY2vwWLgWfGVAAiKQzMng...              28   \n",
       "3  mbwrZ-RS76V1HoJ0bF_Geg, g64lOV39xSLRZO0aQQ6DeQ...            1114   \n",
       "4  RJQTcJVlBsJ3_Yo0JSFQQg, GWt_h78k1CBBkE1NpThGfQ...           13311   \n",
       "\n",
       "   funny_reviews  cool_reviews  n_fans  \\\n",
       "0             17            25       5   \n",
       "1             22            16       4   \n",
       "2              8            10       0   \n",
       "3            279           665      39   \n",
       "4          19356         15319     696   \n",
       "\n",
       "                               years_elite  average_stars  \n",
       "0                           2015,2016,2017           4.03  \n",
       "1                                      NaN           3.63  \n",
       "2                                      NaN           3.71  \n",
       "3                      2015,2016,2017,2018           4.08  \n",
       "4  2006,2007,2008,2009,2010,2011,2012,2013           4.39  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_address</th>\n",
       "      <th>business_city</th>\n",
       "      <th>business_state</th>\n",
       "      <th>business_latitude</th>\n",
       "      <th>business_longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>ON</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>2.5</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                    business_name  \\\n",
       "0  1SWheh84yJXfytovILXOAQ       Arizona Biltmore Golf Club   \n",
       "1  QXAEGFB4oINsVuTFxEYKFQ       Emerald Chinese Restaurant   \n",
       "2  gnKjwL_1w79qoiV3IC_xQQ      Musashi Japanese Restaurant   \n",
       "3  xvX2CttrVhyG2z1dFg_0xw  Farmers Insurance - Paul Lorenz   \n",
       "4  HhyxOkGAM07SRYtlQ4wMFQ              Queen City Plumbing   \n",
       "\n",
       "                 business_address business_city business_state  \\\n",
       "0     2818 E Camino Acequia Drive       Phoenix             AZ   \n",
       "1            30 Eglinton Avenue W   Mississauga             ON   \n",
       "2       10110 Johnston Rd, Ste 15     Charlotte             NC   \n",
       "3   15655 W Roosevelt St, Ste 237      Goodyear             AZ   \n",
       "4  4209 Stuart Andrew Blvd, Ste F     Charlotte             NC   \n",
       "\n",
       "   business_latitude  business_longitude  stars  review_counts  is_open  \\\n",
       "0          33.522143         -112.018481    3.0              5        0   \n",
       "1          43.605499          -79.652289    2.5            128        1   \n",
       "2          35.092564          -80.859132    4.0            170        1   \n",
       "3          33.455613         -112.395596    5.0              3        1   \n",
       "4          35.190012          -80.887223    4.0              4        1   \n",
       "\n",
       "                                          categories  \n",
       "0                                  Golf, Active Life  \n",
       "1  Specialty Food, Restaurants, Dim Sum, Imported...  \n",
       "2                  Sushi Bars, Restaurants, Japanese  \n",
       "3                      Insurance, Financial Services  \n",
       "4  Plumbing, Shopping, Local Services, Home Servi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Subsampling and Train-Test Split\n",
    "#### E.1. FIltering, Undersampling and Dataset Size Considerations\n",
    "The idea behind undersampling is to develop a smaller dataset that is representable, or partially representative of the entire dataset, and expedites the model development cycle. We utilize 20% dataset size generally across the board for fast development iteration and for computation intensive tasks such as hyperparameter tuning. However, it is critical to analyze model performance across dataset sizes since the models’ capability to handle large dataset is an important consideration if it were to be utilized in production. Therefore we investigate two other dataset sizes as well, 50% and 100% of the data size. Whereas 20% of the dataset size is used generally, larger datasets are employed to analyze certain models’ capability to handle large data as well as measure how their performance changes with larger datasets. Finally, we filter our dataset to only active users who have at least 5 reviews since there needs to be at least some data about the user for the model to perform. If a user does not have at least 5 reviews, we will be building out recommendations using cold-start methods.\n",
    "\n",
    "#### E.2 Train Test Split\n",
    "\n",
    "Train-Test split is done in a very straightforward way. We take all the users that made through our filters as described above. Then we simply take the last two reviews of the users. The most recent review becomes our test set and the other second most recent review becomes our validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                 date  rating  \\\n",
      "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
      "1  hk5wpV-_pi5jmDDVPeG8DA  2018-09-14 18:50:19     5.0   \n",
      "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
      "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
      "4  dU-Nt1-LjV9mAgFOVcdAJw  2018-08-15 22:14:18     5.0   \n",
      "\n",
      "                                                text                 user_id  \n",
      "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "1  I highly recommend Arizona Pet Mortuary, David...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "2  First time at this restaurant our server \"Ramo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "3  Such an amazing hospital with friendly staff, ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "4  Went for my yearly GYN exam and was seen by Lo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
      "(918368, 5)\n"
     ]
    }
   ],
   "source": [
    "# Downsample by users\n",
    "user_id_unique = ratings_.user_id.unique()\n",
    "user_id_sample = pd.DataFrame(user_id_unique, columns=['unique_user_id']) \\\n",
    "                    .sample(frac= SAMPLING_RATE, replace=False, random_state=1)\n",
    "\n",
    "ratings_sample = ratings_.merge(user_id_sample, left_on='user_id', right_on='unique_user_id') \\\n",
    "                    .drop(['unique_user_id'], axis=1)\n",
    "print(ratings_sample.head())\n",
    "print(ratings_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 803897 rows, 5 columns in training set.\n",
      "There are 57229 rows, 5 columns in training set.\n",
      "There are 57223 rows, 5 columns in holdout set.\n"
     ]
    }
   ],
   "source": [
    "# hold out last review\n",
    "ratings_user_date = ratings_sample.loc[:, ['user_id', 'date']]\n",
    "ratings_user_date.date = pd.to_datetime(ratings_user_date.date)\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_holdout_ = ratings_sample[index_holdout]\n",
    "ratings_traincv_ = ratings_sample[~index_holdout]\n",
    "\n",
    "ratings_user_date = ratings_traincv_.loc[:, ['user_id', 'date']]\n",
    "index_holdout = ratings_user_date.groupby(['user_id'], sort=False)['date'].transform(max) == ratings_user_date['date']\n",
    "ratings_cv_ = ratings_traincv_[index_holdout]\n",
    "ratings_train_ = ratings_traincv_[~index_holdout]\n",
    "\n",
    "# remove the user that has fake reviews \n",
    "\n",
    "cv_users_del = set(ratings_cv_.user_id) - set(ratings_train_.user_id)\n",
    "holdout_users_del = set(ratings_holdout_.user_id) - set(ratings_train_.user_id)\n",
    "ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(cv_users_del)]\n",
    "ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(holdout_users_del)]\n",
    "\n",
    "# ratings_cv_ = ratings_cv_[~ratings_cv_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "# ratings_holdout_ = ratings_holdout_[~ratings_holdout_.user_id.isin(['HiT9sg9pvDiEVMFHJYihXg'])]\n",
    "\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_train_.shape[0], ratings_train_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in training set.'.format(ratings_cv_.shape[0], ratings_cv_.shape[1]))\n",
    "print('There are {0} rows, {1} columns in holdout set.'.format(ratings_holdout_.shape[0], ratings_holdout_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57223\n"
     ]
    }
   ],
   "source": [
    "# check if we have a enough user sample size (> 50000)\n",
    "number_of_unique_users = len(ratings_train_.user_id.unique())\n",
    "print(number_of_unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Evaluation Metrics\n",
    "\n",
    "A list of evaluation metrics the team uses are:\n",
    "\n",
    "#### F.1. Regression Metrics\n",
    "Regression metrics measure We present four different regression metrics that are measured but we primarily use the RMSE. \n",
    "Root Mean Square Error (RMSE) calculates square rooted sum of square residuals of predictions. It measures numerical difference between all ground-truth ratings and actual ratings in test set.\n",
    "Mean Absolute Error (MAE) calculates sum of absolute residuals. It measures numerical difference between all ground-truth ratings and actual ratings in test set, and more robust to outliers.\n",
    "R-squared measures what percentage of variance in target that is explained by predictions. The higher the value, the better the predictions.\n",
    "\n",
    "#### F.2. Ranking Metrics\n",
    "We first make top 10 recommendations of restaurants for each user and see how relevant the rankings were. First we discuss the metrics then discuss a few caveats to the approach we took to measure them.\n",
    "Inclusion of Last Review in Top 10 Recommendations: We train our model on the training set and make top 10 recommendations to the users. We then look at the test set and see if the user’s latest review was included in that top 10 recommendations. We calculate the proportion of users who received such recommendations. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n \\sum_{i \\in j's top 10}^{10}Rel(i)$$\n",
    "\n",
    "where a recommendation is relevant if it is the latest visited restaurant of the user and n is the total number of users measured. \n",
    "Average Ranking of Latest Restaurant: We train our model on the training set and make a prediction on every single business  and user combination. We then measure the average rankings on that prediction of the latest business that the user visited. In other words we have,\n",
    "\n",
    "$$\\frac{1}{n} * \\sum_j^n\\sum_i^m Rank(i)(1_{i=latest business of j})$$\n",
    "\n",
    "Where the indicator function is one if the restaurant is the last visited restaurant of user j, n is the total number of users and m is the total number of businesses.\n",
    "Some caveats of our approach is that instead of making a prediction on the entire business universe, we make predictions on the businesses that are in the same city as the business of the user’s last review. This makes sense since it would be futile to recommend a restaurant in Los Angeles to a user who is in New York City. This also allows us to reduce the computation time of our recommendation, which is another key advantage we want to have when we serve our model to the users. Additionally, we measure the above two metrics on a subsample of users as opposed to all the users to save computation time, and we also only take subsample of ratings that were positive ratings since we want to measure how good our recommendations are. \n",
    "#### F.3. Coverage\n",
    "We measure the coverage of our recommendations by looking at the proportion of our recommendations that are distinct. In other words, we measure, <br>\n",
    "\n",
    "$$\\frac{number of distinct businesses in all recommendations to the subsample} \n",
    "{number of all recommendations to the subsample}$$\n",
    "\n",
    " \n",
    "#### F.4. Novelty\n",
    "We measure the novelty of our recommendations by simply taking the proportion of businesses in our top ten recommendations that the user hasn’t been to. This is crucial since we don’t want our recommendations to be filled with restaurants that the user has already been to. We measure novelty as simply. \n",
    "\n",
    "#### F.5. Runtime \n",
    "We measure the runtime of the model’s training as well as its prediction time on validation set using Python’s time library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics on Coverage and Serendipity\n",
    "\n",
    "First subsample a group of users that we will measure these metrics from\n",
    "\n",
    "Methodology:\n",
    "    We sample 5 users from each city where the user made the latest review.\n",
    "    These cities must have at least 100 unique businesses\n",
    "    These users must also have made a postive review(above their historical average)to those restaurants.\n",
    "        1. We recommend 10 restaurants to each user\n",
    "        2. We see if their latest restaurant makes it into the top 10 list (Ranking Metric)\n",
    "        3. We see for those 10 x 5 recommendations, how many of them are distinct businesses (Coverage)\n",
    "        4. We see for those top 10 recommendations, how many of them are restaurants they have not visited (Serendipity)\n",
    "    \n",
    "    Additionally, we measure what our ranking was for the latest restaurant that the user visited(Ranking Metric\n",
    "    \n",
    "\n",
    "\n",
    "Criteria: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_holdout = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train_final = ratings_train.append(ratings_val)\n",
    "ratings_entire_df = ratings_train.append(ratings_val).append(ratings_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'date', 'rating', 'text', 'user_id', 'week_day', 'month',\n",
       "       'hour', 'user_name', 'user_review_count', 'user_yelping_since',\n",
       "       'friends', 'useful_reviews', 'funny_reviews', 'cool_reviews', 'n_fans',\n",
       "       'years_elite', 'average_stars', 'business_name', 'address', 'city',\n",
       "       'state', 'latitude', 'longitude', 'stars', 'review_counts', 'is_open',\n",
       "       'categories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_holdout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_city_businesses = ratings_entire_df[['city','business_id']].drop_duplicates()\n",
    "unique_cities = unique_city_businesses.groupby('city').count()['business_id']\n",
    "unique_cities = unique_cities[unique_cities > 100]\n",
    "out = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_holdout[(ratings_holdout['city'] ==city) &\n",
    "                              (ratings_holdout['rating'] >ratings_holdout['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        \n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out = out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = out[['user_id','city','state']]\n",
    "predict_df = predict_df.merge(unique_city_businesses, on = 'city')\n",
    "predict_df.to_csv('data/metric_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df['predictions'] = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(predict_df, validation_subsample, ratings_train_final):\n",
    "    top_10_recs = predict_df.groupby(['user_id','city'])['predictions'].nlargest(10).reset_index()\n",
    "    out = validation_subsample\n",
    "    cnt =0\n",
    "    serendipity = 0\n",
    "    \n",
    "    \n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_2]['business_id']\n",
    "        ###In top 10\n",
    "        if row_values['business_id'] in top_10.values:\n",
    "            cnt+=1\n",
    "        user_history = ratings_train_final[ratings_train_final['user_id'] == row_values['user_id']]    \n",
    "        been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "        serendipity += 1-len(been_there)/10\n",
    "    \n",
    "    top_10 = cnt/len(out)\n",
    "    serendipity = serendipity/len(out)\n",
    "    \n",
    "    predict_df = predict_df.reset_index()\n",
    "    \n",
    "    analysis_df = predict_df.merge(top_10_recs, left_on = ['user_id','city','index'], \\\n",
    "                                   right_on = ['user_id','city','level_2'])\n",
    "    \n",
    "    coverage = (analysis_df.groupby('city')['business_id'].nunique()/50).values.mean()\n",
    "    \n",
    "    predict_df['rankings']=predict_df.groupby(['city','user_id'])['predictions']. \\\n",
    "                                                        rank(method=\"first\",ascending = False)\n",
    "    running_rankings =0\n",
    "    for row in out.iterrows():\n",
    "        row_values = row[1]\n",
    "        user_recs = predict_df[(predict_df['user_id']==row_values['user_id'])\n",
    "                            &(predict_df['city']==row_values['city'])\n",
    "                             & (predict_df['business_id']==row_values['business_id'])\n",
    "                              ]\n",
    "        assert len(user_recs)==1\n",
    "        running_rankings += user_recs['rankings'].sum()\n",
    "\n",
    "    avg_rank = running_rankings / len(out)\n",
    "    print(top_10, coverage, serendipity, avg_rank)\n",
    "    \n",
    "    return top_10, coverage, serendipity, avg_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Methods\n",
    "\n",
    "A list of models that the team attempts are: \n",
    "\n",
    "* Bias Baseline\n",
    "* Collaborative Filtering Baseline: SVD\n",
    "* Collective Matrix Factorization (CMF)\n",
    "* Content-Based Filtering (CBF)\n",
    "* Field-aware Factorization Machine (FFM) \n",
    "* Deep Learning Model\n",
    "\n",
    "_**Note**_: the team has run algorithms on **20%, 50%, 100%** training data respectively. But for readability purpose, only result on 20% data is displayed in this notebook. For full result, please refer to a result table enclosed in **pdf report**.\n",
    "\n",
    "### Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: Bias Baseline\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
    "\\lambda \\left(b_u^2 + b_i^2 \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset\n",
    "from surprise import BaselineOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':3}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':5}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':7}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3491711762452891"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsl_options = {'method': 'als', 'n_epochs':9}\n",
    "bias_baseline = BaselineOnly(bsl_options)\n",
    "algo.fit(train_sr_20)\n",
    "predictions = algo.test(val_sr_20)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: It seems different hyperparameters all performs the same result; the team just uses default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "**Note**: the team has evaluated this and the following algorithms on **20%, 50%, 100%** training data respectively. For readability, only result on 20% data is displayed. For full result, please refer to a result table enclosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "--- 2.0564420223236084 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 20%\n",
    "start_time = time.time()\n",
    "bias_baseline.fit(train_sr_20)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3545\n",
      "R^2 (with 20% data):  0.19799189125456051\n",
      "MAE (with 20% data):  1.127068744947832\n",
      "--- 0.08617496490478516 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 20%\n",
    "bbase_p = bias_baseline.test(test_sr_20)\n",
    "start_time = time.time()\n",
    "bbase_20_df = pd.DataFrame(bbase_p, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "accuracy.rmse(bbase_p)\n",
    "print('R^2 (with 20% data): ', r2_score(bbase_20_df.rating , bbase_20_df.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(bbase_20_df.rating, bbase_20_df.pred_rating))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "algo = BaselineOnly()\n",
    "eval_before_50 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "algo.fit(train_sr_20)\n",
    "eval_pred_20 = algo.test(eval_sr_20)\n",
    "#accuracy.rmse(predictions_50)\n",
    "baseline_20 = pd.DataFrame(eval_pred_20, columns = ['userId','itemId','rating','pred_rating','x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10714285714285714 0.503095238095238 0.9697619047619042 528.3333333333334\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: Collaborative Filtering via SVD\n",
    "\n",
    "Matrix factorization is a class of collaborative filtering algorithms. The general idea behind matrix factorization is that there can exist a lower dimensional latent space of features in which users and items can be represented such that the interaction between them can be obtained by simply dot producing the corresponding dense vectors in that space. In short, it decomposes a m*n user-item interaction matrix into two m*k and k*n matrices, sharing a joint latent vector space, where m represents the number of users, and n represents the number of items. In terms of its outcome, we are likely to observe that close users in terms of preferences as well as close items in terms of characteristics can have close representations in the latent space.\n",
    "\n",
    "The mathematical overview is as follows:\n",
    "Given a n*m matrix, such that . X is the user matrix where rows represent the n users and Y is the item matrix where rows represent the m items. We want to search for the dot product of matrices X and Y that best approximate the existing interactions; i.e., we want to find X and Y that minimize the “rating reconstruction error”:\n",
    "\n",
    "$$ (X,Y) = argmin_{X,Y} \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2$$\n",
    "\n",
    "Adding a regularization term, we can also get:\n",
    "\n",
    "$$(X,Y) = argmin_{X,Y} ½ \\sum_{(i,j) \\in E} [(X_i)(Y_j)^T − M_{ij}]^2 + \\lambda/2(\\sum_{i,k}(X_{ik})^2 + \\sum_{j,k}(Y_{jk})^2)$$\n",
    "\n",
    "In general, we obtain the matrices X and Y following a gradient descent optimization process. And once the matrices are obtained, we can predict the ratings simply by multiplying the user vector by any item vector.\n",
    "\n",
    "In this Yelp Rating Challenge, we used the python surprise package to implement MF. The MF algorithm there uses the SVD approach, which is essentially \n",
    "\n",
    "$$ P_{m * n} = U_{m * m} \\sum_{m * n} V_{n * n}$$\n",
    "\n",
    "There, the prediction is\n",
    " $$\\hat(r_{ui}) = \\mu + b_u + b_i + (q_i)^T p_u $$\n",
    " \n",
    "and the regularized squared error that needs to be minimized is \n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} (r_{ui} − \\hat(r_{ui}))^2 + \\lambda(b^2_{i} + b^2_{u} + ||q_i||^2 + ||p_u||^2)$$\n",
    "\n",
    "As the way the package is designed, we tuned on n_epochs, lr_all and leg_all to get an optimal hyperparameter set, where n_epochs is the number of iterations of the SGD (stochastic gradient descent) procedure, lr_all is the learning rate for all parameters, and reg_all is the regularization term for all parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cmfrec\n",
    "# !pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzljohn18/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from cmfrec import CMF\n",
    "from surprise import KNNBasic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "              'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_test = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test = ratings_test.loc[ratings_test.business_id.isin(ratings_train.business_id)]\n",
    "ratings_val = ratings_val.loc[ratings_val.business_id.isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ratings_train.loc[:,['user_id', 'business_id', 'rating']]\n",
    "trainset.columns = ['userID', 'itemID','rating']\n",
    "valset = ratings_val.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "valset.columns = ['userID', 'itemID','rating']\n",
    "testset = ratings_holdout.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "testset.columns = ['userID', 'itemID','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale = (0.0, 5.0))\n",
    "train_data = Dataset.load_from_df(trainset[['userID','itemID','rating']], reader)\n",
    "val_data = Dataset.load_from_df(valset[['userID','itemID','rating']], reader)\n",
    "test_data = Dataset.load_from_df(testset[['userID','itemID','rating']], reader)\n",
    "\n",
    "train_sr = train_data.build_full_trainset()\n",
    "val_sr_before = val_data.build_full_trainset()\n",
    "val_sr = val_sr_before.build_testset()\n",
    "test_sr_before = test_data.build_full_trainset()\n",
    "test_sr = test_sr_before.build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4171\n",
      "RMSE: 1.4166\n",
      "RMSE: 1.4172\n",
      "RMSE: 1.4012\n",
      "RMSE: 1.4023\n",
      "RMSE: 1.4032\n",
      "RMSE: 1.3794\n",
      "RMSE: 1.3805\n",
      "RMSE: 1.3820\n",
      "RMSE: 1.4048\n",
      "RMSE: 1.4047\n",
      "RMSE: 1.4054\n",
      "RMSE: 1.3888\n",
      "RMSE: 1.3888\n",
      "RMSE: 1.3902\n",
      "RMSE: 1.3643\n",
      "RMSE: 1.3664\n",
      "RMSE: 1.3681\n",
      "RMSE: 1.3897\n",
      "RMSE: 1.3906\n",
      "RMSE: 1.3914\n",
      "RMSE: 1.3721\n",
      "RMSE: 1.3733\n",
      "RMSE: 1.3749\n",
      "RMSE: 1.3496\n",
      "RMSE: 1.3513\n",
      "RMSE: 1.3529\n"
     ]
    }
   ],
   "source": [
    "RMSE_tune = {}\n",
    "n_epochs = [5, 7, 10]  # the number of iteration of the SGD procedure\n",
    "lr_all = [0.002, 0.003, 0.005] # the learning rate for all parameters\n",
    "reg_all =  [0.4, 0.5, 0.6] # the regularization term for all parameters\n",
    "\n",
    "for n in n_epochs:\n",
    "    for l in lr_all:\n",
    "        for r in reg_all:\n",
    "            algo = SVD(n_epochs = n, lr_all = l, reg_all = r)\n",
    "            algo.fit(train_sr)\n",
    "            predictions = algo.test(val_sr)\n",
    "            RMSE_tune[n,l,r] = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 0.005, 0.4)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "min(RMSE_tune.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The best combination is when n_epochs = 10, lr_all = 0.005, reg_all = 0.4, and the RMSE score is 1.3496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test on the optimal parameter\n",
    "start_time = time.time()\n",
    "algo_real = SVD(n_epochs = 10, lr_all = 0.005, reg_all = 0.4)\n",
    "algo_real.fit(train_sr)\n",
    "predictions = algo_real.test(test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 33.18766689300537 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.398543135413844"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.1848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1847820428584857"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'city', 'state', 'business_id'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_20 = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "predict_df_20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate coverage and serendipity metrics, use evaluation set created earlier.\n",
    "predict_df_20 = pd.read_csv('data/metric_sample.csv', index_col=0)\n",
    "predict_df_20['predictions'] = 2.5 # fill in this value temporally\n",
    "eval_20 = Dataset.load_from_df(predict_df_20[['user_id','business_id','predictions']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_before_20 = eval_20.build_full_trainset()\n",
    "eval_sr_20 = eval_before_20.build_testset()\n",
    "eval_pred_20 = algo_real.test(eval_sr_20)\n",
    "\n",
    "baseline_20 = pd.DataFrame(eval_pred_20, columns = ['userId','itemId','rating','pred_rating','x'])\n",
    "predict_df_20['predictions'] = baseline_20.pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10930232558139535 0.4932558139534884 0.9762790697674414 518.7511627906977\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "### (i) Collective Matrix Factorization (CMF)\n",
    "\n",
    "Collective Matrix Factorization method decomposes two matrices $X$ and $Y$ into three matrices $U$, $V$, and $Z$ such that $X \\approx f(UV^T)$ and $ Y \\approx f(VZ^T)$, where f is either the identity or sigmoid function. This allows us to include additional features of our users or items that might help optimize the result of personalization. \n",
    "\n",
    "To beat the baseline (which in our case, is Matrix Factorization), we first tried the Collective Matrix Factorization technique. This means that, in addition to the ratings, we also want to include additional features on either the items or the users, to see how the model perform. Before we discussed what additional features we included, we might say a few words on the method and its mathematical intuition.\n",
    "\n",
    "Similar to Matrix Factorization, Collective Matrix Factorization also aims to decompose, but in this time, we are decomposing two matrices $X$ and $Y$ into three matrices $U$, $V$, and $Z$, such that $X \\approx f(UV^T)$ and $Y \\approx f(VZ^T)$. Here, X can be the same matrix we see in the case of Matrix Factorization, i.e., a simple user-item matrix filled by ratings. And matrix Y is the matrix of additional features. It can be on user or on item. And the features can be one-hot encoded (i.e., categorical) or numerical. For example, in our case, we both involved state location for businesses (which is a categorical feature) and the average rating (which is a numerical feature).\n",
    "\n",
    "Collective Matrix Factorization also has a function to minimize, and we will explain the function through the lens of the python cmfrec package, which is the package that we used to implement Collective Matrix Factorization. The function to minimize is as follows:\n",
    "\n",
    "$ argmin_{A, B, C, D, U_b, I_b} \\lVert (X − \\mu − U_b − I_b − AB^T)I_{x} \\lVert^2 + \\lVert U − AC^T \\lVert^2 + \\lVert I − BD^T \\lVert^2 + \\lambda (\\lVert A\\lVert^2 + \\lVert B \\lVert^2 + \\lVert C \\lVert^2 + \\lVert D \\lVert^2  + \\lVert U_b \\lVert^2 + \\lVert I_b \\lVert^2)$\n",
    "\n",
    "Where X is the ratings matrix, I is the item-attribute matrix, U is the user-attribute matrix, and A,B,C,D are lower-dimensional matrices. $ |X| $, $|I|$, $|U|$ are the number of non-missing entries in each matrix. And $ Ubias_{u}$ and $Ibias_{i}$ are user and item biases. Thus, for such a reason, when tuning, we tune w_main and w_item when we are including additional item features and we tune w_main and w_user when we are including additional user features.\n",
    "\n",
    "Now, with CMF, we implemented **3** approaches in total- **state average rating, state location, and user average rating**. Please refer to detail as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users_\n",
    "businesses = business_\n",
    "\n",
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users, on = 'user_id')\n",
    "    df = df.merge(businesses, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20 = trainset_20\n",
    "X_train_20.columns = ['UserId','ItemId','Rating']\n",
    "\n",
    "X_test_20 = testset_20\n",
    "X_test_20.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1: CMF on User, Business and State Average Rating**<br>\n",
    "\n",
    "We first included state average rating as an item additional feature. Since our objective here is to predict the last rating for each active user, we wanted to see if location can be a useful means to bring closer the predicted ratings to the actual ones. To better utilize the location feature though, we came up with two approaches. One is to calculate the state average rating, and the other is to use state location as a categorical feature. With the state average rating, we wanted to observe if some states can have a higher average rating than others. This means that either the restaurants in that state are significantly better or the users in that state are more lenient and friendly. From EDA alone, the average rating seems to make some sense, as we observed that California has the highest state average rating (though with much fewer data observations) and New York has the lowest (same, with much fewer data observations than AZ and NV). This might suggest that, even though New York (especially manhattan area) is known as a food hub, the yelp users here can be a little picky and critical, whereas the users in CA can be more friendly. By all means though, we wanted to see if this “secondary” information that we generated for ourselves can help better predict the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state average rating\n",
    "state_avg_20 = pd.DataFrame(ratings_train_20.groupby(\"state\").rating.mean())\n",
    "state_avg_20.columns = ['state_avg']\n",
    "train_state_avg_20 = ratings_train_20.merge(state_avg_20, on = \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item additional info: state average\n",
    "item_avg_20 = train_state_avg_20.loc[:,['business_id','state_avg']]\n",
    "item_avg_20.columns = ['ItemId','state_avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 0.951550\n",
      "  Number of iterations: 378\n",
      "  Number of functions evaluations: 429\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 0.951519\n",
      "  Number of iterations: 307\n",
      "  Number of functions evaluations: 351\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.950972\n",
      "  Number of iterations: 99\n",
      "  Number of functions evaluations: 110\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.175318\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1077\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.163224\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1067\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.151421\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1050\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.763368\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1060\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.714961\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1053\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.669455\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1043\n"
     ]
    }
   ],
   "source": [
    "tune = {}\n",
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_item = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the item attributes matrix\n",
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_item:\n",
    "        model = CMF(w_main = m, w_item = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_avg_20))\n",
    "        prediction = model.predict(X_val_20.UserId, X_val_20.ItemId)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.Rating)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.5, 10.0): 1.4095315793715344,\n",
       " (0.5, 5.0): 1.4095280852831702,\n",
       " (0.5, 0.5): 1.4095288030679354,\n",
       " (5.0, 10.0): 1.325081092000988,\n",
       " (5.0, 5.0): 1.3249653813051294,\n",
       " (5.0, 0.5): 1.325319757480712,\n",
       " (10.0, 10.0): 1.3303194326966257,\n",
       " (10.0, 5.0): 1.3291943801778212,\n",
       " (10.0, 0.5): 1.329444185787565}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Best param: w_main = 5.0, w_item = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.163018\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cmfrec.CMF at 0x1b80676f60>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20%\n",
    "model = CMF(w_main = 5.0, w_item = 5.0, random_seed = 1)\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_avg_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 824.5971691608429 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_20 = X_test_20.loc[X_test_20.ItemId.isin(X_train_20.ItemId)]\n",
    "X_test_50 = X_test_50.loc[X_test_50.ItemId.isin(X_train_50.ItemId)]\n",
    "X_test_100 = X_test_100.loc[X_test_100.ItemId.isin(X_train_100.ItemId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (with 20% data):  1.3495188130540272\n",
      "R^2 (with 20% data):  0.18677513147304292\n",
      "MAE (with 20% data):  1.1202827761356042\n"
     ]
    }
   ],
   "source": [
    "state_prediction_20 = model.predict(X_test_20.UserId, X_test_20.ItemId)\n",
    "X_test_20['pred_rating'] = state_prediction_20\n",
    "print('RMSE (with 20% data): ', np.sqrt(np.mean((X_test_20.pred_rating - X_test_20.Rating)**2)))\n",
    "print('R^2 (with 20% data): ', r2_score(X_test_20.Rating ,X_test_20.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(X_test_20.Rating, X_test_20.pred_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>business_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_rUho9z3p91M1r9hqA7Bg</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>8AW0koYMDa1PlJMOE-b2-g</td>\n",
       "      <td>3.237104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_rUho9z3p91M1r9hqA7Bg</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>-YGQwikbX2fXUIjyegR7pw</td>\n",
       "      <td>3.680566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_rUho9z3p91M1r9hqA7Bg</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>5Kh5i4VhXj-Leg8gujIzjQ</td>\n",
       "      <td>3.642935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_rUho9z3p91M1r9hqA7Bg</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>Wl1oOVbtK4I9vRKoaSKYiQ</td>\n",
       "      <td>3.359295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_rUho9z3p91M1r9hqA7Bg</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>OxSaGGTmIujsjDpDqwyGPQ</td>\n",
       "      <td>3.437118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id  city state             business_id  predictions\n",
       "0  4_rUho9z3p91M1r9hqA7Bg  Ajax    ON  8AW0koYMDa1PlJMOE-b2-g     3.237104\n",
       "1  4_rUho9z3p91M1r9hqA7Bg  Ajax    ON  -YGQwikbX2fXUIjyegR7pw     3.680566\n",
       "2  4_rUho9z3p91M1r9hqA7Bg  Ajax    ON  5Kh5i4VhXj-Leg8gujIzjQ     3.642935\n",
       "3  4_rUho9z3p91M1r9hqA7Bg  Ajax    ON  Wl1oOVbtK4I9vRKoaSKYiQ     3.359295\n",
       "4  4_rUho9z3p91M1r9hqA7Bg  Ajax    ON  OxSaGGTmIujsjDpDqwyGPQ     3.437118"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bowenzhou/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/bowenzhou/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/bowenzhou/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.163224\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1067\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 7.329798\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1069\n"
     ]
    }
   ],
   "source": [
    "model = CMF(w_main = 5.0, w_item = 5.0, random_seed = 1)\n",
    "model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(item_avg_20))\n",
    "predict_df_20['predictions'] = model.predict(predict_df_20.user_id, predict_df_20.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13095238095238096 0.2609523809523809 0.9695238095238087 484.93809523809523\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2: CMF on State Location as feature**<br>\n",
    "\n",
    "\n",
    "With the second approach on the location feature, we simply fed in the one-hot encoded columns. This time, we just want to see if the location itself, as a categorical feature, can bring us any closer to the actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode\n",
    "df_state_20 = pd.get_dummies(ratings_train_20.state)\n",
    "\n",
    "state_loc_20 = ratings_train_20.loc[:,['business_id']]\n",
    "state_loc_20.columns = ['ItemId']\n",
    "state_loc_20 = state_loc_20.join(df_state_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 1.502233\n",
      "  Number of iterations: 48\n",
      "  Number of functions evaluations: 59\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 1.345407\n",
      "  Number of iterations: 49\n",
      "  Number of functions evaluations: 61\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.946406\n",
      "  Number of iterations: 46\n",
      "  Number of functions evaluations: 58\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.707548\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1050\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.544841\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1054\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.147314\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1050\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 9.238955\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1047\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 9.068397\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1045\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.660084\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1044\n"
     ]
    }
   ],
   "source": [
    "tune_2 = {}\n",
    "\n",
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_item = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the item attributes matrix\n",
    "\n",
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_item:\n",
    "        model = CMF(w_main = m, w_item = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(state_loc_20),\\\n",
    "            cols_bin_item=[cl for cl in state_loc_20.columns if cl != 'ItemId'])\n",
    "        prediction = model.predict(X_val_20.UserId, X_val_20.ItemId)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune_2[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.Rating)**2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.5, 10.0): 1.4091453521706512,\n",
       " (0.5, 5.0): 1.4093103607696387,\n",
       " (0.5, 0.5): 1.4096837868418028,\n",
       " (5.0, 10.0): 1.3255848781332549,\n",
       " (5.0, 5.0): 1.3253454951331827,\n",
       " (5.0, 0.5): 1.3255074066479042,\n",
       " (10.0, 10.0): 1.3287372061500575,\n",
       " (10.0, 5.0): 1.3285188529615481,\n",
       " (10.0, 0.5): 1.3288867514741107}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best param: w_main = 5.0, w_item = 5.0\n",
    "tune_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.545013\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1050\n",
      "--- 846.5123789310455 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# 20%\n",
    "X_test_20 = X_test_20.loc[X_test_20.ItemId.isin(X_train_20.ItemId)]\n",
    "model = CMF(w_main = 5.0, w_item = 5.0, random_seed = 1)\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(state_loc_20),\\\n",
    "            cols_bin_item=[cl for cl in state_loc_20.columns if cl != 'ItemId'])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (with 20% data):  1.3496651740914456\n",
      "R^2 (with 20% data):  0.18659872653728782\n",
      "MAE (with 20% data):  1.1201697481001782\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "loc_prediction_20 = model.predict(X_test_20.UserId, X_test_20.ItemId)\n",
    "X_test_20['pred_rating'] = loc_prediction_20\n",
    "print('RMSE (with 20% data): ', np.sqrt(np.mean((X_test_20.pred_rating - X_test_20.Rating)**2)))\n",
    "print('R^2 (with 20% data): ', r2_score(X_test_20.Rating , X_test_20.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(X_test_20.Rating , X_test_20.pred_rating))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.545013\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1050\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 7.754635\n",
      "  Number of iterations: 424\n",
      "  Number of functions evaluations: 457\n"
     ]
    }
   ],
   "source": [
    "model = CMF(w_main = 5.0, w_item = 5.0, random_seed = 1)\n",
    "model.fit(ratings = deepcopy(X_train_20), item_info = deepcopy(state_loc_20),\\\n",
    "            cols_bin_item=[cl for cl in state_loc_20.columns if cl != 'ItemId'])\n",
    "predict_df_20['predictions'] = model.predict(predict_df_20.user_id, predict_df_20.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13095238095238096 0.2602380952380952 0.9692857142857138 473.73571428571427\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 3: CMF on User Average Rating as feature**<br>\n",
    "\n",
    "Lastly, we also did a user average rating for the user info. This is similar to the state average rating approach, but we just wanted to see if by directly analyzing the users’ behaviors, that normally how critical they are with the restaurants, can help us better understand them and have a better predicted result than other approaches. To notice here though, we didn’t use the given average rating, since that would include the last rating that we were trying to predict; thus, we had to hand-calculated the average rating again on the training set. We also wanted to include the yelp spending time, that is the time they been using yelp and see if this can help us better predict the result. But due to the time constraint, we are not able to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get state average rating\n",
    "user_avg_20 = pd.DataFrame(ratings_train_20.groupby(\"user_id\").rating.mean())\n",
    "user_avg_20.columns = ['user_avg']\n",
    "user_avg_20 = ratings_train_20.merge(user_avg_20, on = \"user_id\")\n",
    "\n",
    "user_avg_50 = pd.DataFrame(ratings_train_50.groupby(\"user_id\").rating.mean())\n",
    "user_avg_50.columns = ['user_avg']\n",
    "user_avg_50 = ratings_train_50.merge(user_avg_50, on = \"user_id\")\n",
    "\n",
    "user_avg_100 = pd.DataFrame(ratings_train_100.groupby(\"user_id\").rating.mean())\n",
    "user_avg_100.columns = ['user_avg']\n",
    "user_avg_100 = ratings_train_100.merge(user_avg_100, on = \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user additional info: user average\n",
    "user_info_20 = user_avg_20.loc[:,['user_id','user_avg']]\n",
    "user_info_20.columns = ['UserId','state_avg']\n",
    "\n",
    "user_info_50 = user_avg_50.loc[:,['user_id','user_avg']]\n",
    "user_info_50.columns = ['UserId','state_avg']\n",
    "\n",
    "user_info_100 = user_avg_100.loc[:,['user_id','user_avg']]\n",
    "user_info_100.columns = ['UserId','state_avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_3 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_main = [0.5, 5.0, 10.0] # weight assign to the MRSE in factorization of the ratings matrix\n",
    "w_user = [0.5, 5.0, 10.0][::-1] # weight assign to the MRSE in factorization of the user attributes matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 0.912842\n",
      "  Number of iterations: 463\n",
      "  Number of functions evaluations: 530\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "  Objective function value: 0.912821\n",
      "  Number of iterations: 378\n",
      "  Number of functions evaluations: 436\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "  Objective function value: 0.912564\n",
      "  Number of iterations: 131\n",
      "  Number of functions evaluations: 147\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.144692\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1085\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.126369\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1064\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.111398\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1046\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.808755\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1096\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.702769\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1061\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 8.629791\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1034\n"
     ]
    }
   ],
   "source": [
    "# tuning\n",
    "for m in w_main:\n",
    "    for i in w_user:\n",
    "        model = CMF(w_main = m, w_user = i, random_seed = 1)\n",
    "        model.fit(ratings = deepcopy(X_train_20), user_info = deepcopy(user_info_20))\n",
    "        prediction = model.predict(X_val_20.UserId, X_val_20.ItemId)\n",
    "        X_val_20['pred_rating'] = prediction\n",
    "        tune_3[m,i] = np.sqrt(np.mean((X_val_20.pred_rating - X_val_20.Rating)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.5, 10.0): 1.4093796110241372,\n",
       " (0.5, 5.0): 1.4093675138007682,\n",
       " (0.5, 0.5): 1.409379510028695,\n",
       " (5.0, 10.0): 1.3243942755866573,\n",
       " (5.0, 5.0): 1.3248028150484914,\n",
       " (5.0, 0.5): 1.3249520550165859,\n",
       " (10.0, 10.0): 1.3290478291921621,\n",
       " (10.0, 5.0): 1.3281606768811247,\n",
       " (10.0, 0.5): 1.327849756600441}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best param: w_main = 5.0, w_user = 10.0\n",
    "tune_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_50 = ratings_holdout_50.iloc[:, 0:3]\n",
    "testset_50.columns = ['userID', 'itemID','rating']\n",
    "\n",
    "X_test_50 = testset_50\n",
    "X_test_50.columns = ['UserId','ItemId','Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_20 = X_test_20.loc[X_test_20.ItemId.isin(X_train_20.ItemId)]\n",
    "X_test_50 = X_test_50.loc[X_test_50.ItemId.isin(X_train_50.ItemId)]\n",
    "X_test_100 = X_test_100.loc[X_test_100.ItemId.isin(X_train_100.ItemId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.150842\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cmfrec.CMF at 0x1bf451cac8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20%\n",
    "model = CMF(w_main = 5.0, w_user = 10.0, random_seed = 1)\n",
    "start_time = time.time()\n",
    "model.fit(ratings = deepcopy(X_train_20), user_info = deepcopy(user_info_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 825.6145222187042 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (with 20% data):  1.349372260486216\n",
      "R^2 (with 20% data):  0.1869517480867452\n",
      "MAE (with 20% data):  1.1194434259908603\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "user_prediction_20 = model.predict(X_test_20.UserId, X_test_20.ItemId)\n",
    "X_test_20['pred_rating'] = user_prediction_20\n",
    "print('RMSE (with 20% data): ', np.sqrt(np.mean((X_test_20.pred_rating - X_test_20.Rating)**2)))\n",
    "print('R^2 (with 20% data): ', r2_score(X_test_20.Rating , X_test_20.pred_rating))\n",
    "print('MAE (with 20% data): ', mean_absolute_error(X_test_20.Rating , X_test_20.pred_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.562385082244873 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 6.141467\n",
      "  Number of iterations: 1000\n",
      "  Number of functions evaluations: 1066\n"
     ]
    }
   ],
   "source": [
    "model = CMF(w_main = 5.0, w_user = 10.0, random_seed = 1)\n",
    "model.fit(ratings = deepcopy(X_train_20), user_info = deepcopy(user_info_20))\n",
    "predict_df_20['predictions'] = model.predict(predict_df_20.user_id, predict_df_20.business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1357142857142857 0.2580952380952381 0.9699999999999993 470.53333333333336\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df_20, out_20, ratings_train_final_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Content Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1: Predict rating via user reviews**<br>\n",
    "\n",
    "Implemented according to **CF-MCM (User-Item based)** approach in this following paper: https://arxiv.org/pdf/1607.00024.pdf\n",
    "\n",
    "The idea of this model is to predict the rating of a review given by user _U_ to restaurant _R_ by comparing the review to the ones written by other users who have also reviewed restaurant _R_. The predicted rating is calculated by taking a weighted average of other users' ratings, where the weight is calculated based on similarity of reviews. Here is the detail: \n",
    "\n",
    "<img src=\"image/formula.png\" width=\"700\">\n",
    "\n",
    "Two users'ratings are compared in the following manner: first aggregate all reviews user _u_ and user _a_ have written; then bucket them according to ratings given (from r = 1.0 to 5.0); finally find the max of all 5 pairs of similarity. \n",
    "\n",
    "<img src=\"image/text_comparison.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jzljohn18/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings_train_.copy()\n",
    "ratings_cv = ratings_cv_.copy()\n",
    "ratings_holdout = ratings_holdout_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # substitute some irregular context \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # remove punctuations from each word\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    \n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    # Replace slang terms\n",
    "    # Word stemming\n",
    "    \n",
    "#     porter = PorterStemmer()\n",
    "#     text = [porter.stem(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "ratings_train['text'] = ratings_train['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>say office really together organized friendly ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30Q5xBagQHmkwp8Q9I1FCg</td>\n",
       "      <td>2018-02-03 23:27:43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>first time restaurant server ramone asked firs...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UtWngqS-WloIY_A53W5K-Q</td>\n",
       "      <td>2016-02-18 06:42:16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>amazing hospital friendly staff nurses doctors...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76aGN20BrGWvAlYfPGVv_A</td>\n",
       "      <td>2016-04-05 15:25:11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>let start saying grew father mechanic retired ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FUcwrXBb_ljg3LgTqt6F2g</td>\n",
       "      <td>2018-08-15 22:07:13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>love pharmacy compounded prescription cats hyp...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
       "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
       "5  76aGN20BrGWvAlYfPGVv_A  2016-04-05 15:25:11     5.0   \n",
       "6  FUcwrXBb_ljg3LgTqt6F2g  2018-08-15 22:07:13     5.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  say office really together organized friendly ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "2  first time restaurant server ramone asked firs...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  amazing hospital friendly staff nurses doctors...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "5  let start saying grew father mechanic retired ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  love pharmacy compounded prescription cats hyp...  n6-Gk65cPZL6Uz8qRm3NYw  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatnate all reviews for each business for each rating value\n",
    "ratings_train_sub = ratings_train[['user_id', 'business_id', 'rating', 'text']]\n",
    "# ratings_train_sub['ct'] = 0\n",
    "userid_df = ratings_train_sub.groupby(['user_id', 'rating']).agg({'text': ' '.join, 'business_id': 'count'})\n",
    "# retain a copy of indexed dataset\n",
    "businessid_userid_df = ratings_train_sub.set_index(['user_id', 'business_id', 'rating'])\n",
    "\n",
    "## Learn TF-IDF vector representation for each concatenated review\n",
    "# which will be used for similarity calculation\n",
    "userid_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, analyzer='word', \\\n",
    "                             stop_words='english', max_features=50)\n",
    "userid_tfidf_fit = userid_vectorizer.fit(userid_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm implementation \n",
    "\n",
    "def predict_approach_1(selected_uid, selected_bid): \n",
    "    # cosine similarity\n",
    "    def text_similarity(v1, v2):\n",
    "        return np.dot(v1, v2.T).toarray()[0][0]\n",
    "    \n",
    "    pred_rating = -1\n",
    "\n",
    "    users_reviews = userid_df\n",
    "\n",
    "    selected_business = businessid_userid_df.xs(selected_bid, level='business_id').reset_index()\n",
    "    users_who_review_business = selected_business.user_id\n",
    "\n",
    "    user_reviews_userID = users_reviews.xs(selected_uid, level='user_id')\n",
    "\n",
    "    w_ui_u_vec = []\n",
    "    for ui in users_who_review_business:\n",
    "        user_reviews_ui = users_reviews.xs(ui, level='user_id')\n",
    "        rating_range = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "        valid_rating_range_1 = user_reviews_userID.reset_index()['rating'].tolist()\n",
    "        valid_rating_range_2 = user_reviews_ui.reset_index()['rating'].tolist()\n",
    "        w_ui_u = -100\n",
    "\n",
    "        for r in rating_range:\n",
    "            if (r not in valid_rating_range_1) or (r not in valid_rating_range_2):\n",
    "                continue\n",
    "            else:\n",
    "                user_reviews_r = user_reviews_userID.loc[r, 'text']\n",
    "                ui_reviews_r = user_reviews_ui.loc[r, 'text']\n",
    "                user_reviews_r_vec = userid_tfidf_fit.transform([user_reviews_r])\n",
    "                ui_reviews_r_vec = userid_tfidf_fit.transform([ui_reviews_r])\n",
    "                # similarity_r = text_similarity(user_reviews_r, ui_reviews_r)\n",
    "                similarity_r = text_similarity(user_reviews_r_vec, ui_reviews_r_vec)\n",
    "                if similarity_r > w_ui_u:\n",
    "                    w_ui_u = similarity_r\n",
    "        w_ui_u_vec.append(w_ui_u)\n",
    "\n",
    "    tmp = user_reviews_userID.reset_index()\n",
    "    r_u_bar = sum(tmp.rating * tmp.business_id) / sum(tmp.business_id) # business_id is number of ratings \n",
    "                                                                        # that has correpsonding value \n",
    "    if sum(w_ui_u_vec) == 0:\n",
    "        pred_rating = r_u_bar + sum(w_ui_u_vec \\\n",
    "                                * (selected_business.rating - np.mean(selected_business.rating))) \\\n",
    "                                * 1.0 / sum(w_ui_u_vec)\n",
    "    else:\n",
    "        pred_rating = r_u_bar\n",
    "    return pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ratings_cv[ratings_cv['user_id'].isin(ratings_train.user_id)].reset_index()\n",
    "ratings_cv_filtered = tmp_df[tmp_df['business_id'].isin(ratings_train.business_id)]\n",
    "\n",
    "tmp_df = ratings_holdout[ratings_holdout['user_id'].isin(ratings_train.user_id)].reset_index()\n",
    "ratings_holdout_filtered = tmp_df[tmp_df['business_id'].isin(ratings_train.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1172it [07:08,  3.02it/s]/home/jzljohn18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "53225it [4:48:43,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approach 1 takes 288.7285438974698 minutes to run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "approach_1_start = time.time()\n",
    "\n",
    "predict_df_1 = ratings_cv_filtered.loc[:, ['user_id', 'business_id', 'rating']]\n",
    "predict_df_1['pred_rating'] = -1\n",
    "\n",
    "for (u, b) in tqdm(predict_df_1.iterrows(), position=0, leave=True):\n",
    "    try:\n",
    "        pred = predict_approach_1(b.user_id, b.business_id)\n",
    "        predict_df_1.loc[u, 'pred_rating'] = pred \n",
    "    except AttributeError:\n",
    "        if sum(predict_df_1.pred_rating == -1) == 0:\n",
    "            print('Finished')\n",
    "        else:\n",
    "            print('Error: Not Finised')\n",
    "            print('Currently at u = {}'.format(u))\n",
    "approach_1_duration = (time.time() - approach_1_start) * 1.0 / 60 \n",
    "print('The approach 1 takes {} minutes to run.'.format(approach_1_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>pred_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>dU-Nt1-LjV9mAgFOVcdAJw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "      <td>d10IxZPirVJlOSpdRZJczA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "      <td>BxJAl-LoiSiIHonoGzVu7Q</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "      <td>gcouHCQrswvakJ6xSEKtFQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.124476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TpyOT5E16YASd7EWjLQlrw</td>\n",
       "      <td>sYSlKRCWmVeQr1hA6-WUzw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.725275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  rating  pred_rating\n",
       "0  n6-Gk65cPZL6Uz8qRm3NYw  dU-Nt1-LjV9mAgFOVcdAJw     5.0     3.857143\n",
       "1  d6xvYpyzcfbF_AZ8vMB7QA  d10IxZPirVJlOSpdRZJczA     1.0     2.800000\n",
       "2  sG_h0dIzTKWa3Q6fmb4u-g  BxJAl-LoiSiIHonoGzVu7Q     2.0     3.400000\n",
       "3  FIk4lQQu1eTe2EpzQ4xhBA  gcouHCQrswvakJ6xSEKtFQ     4.0     4.124476\n",
       "4  TpyOT5E16YASd7EWjLQlrw  sYSlKRCWmVeQr1hA6-WUzw     4.0     3.725275"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53225, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53162, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows without predictions\n",
    "predict_df_1_NA_removed = predict_df_1.loc[~predict_df_1.pred_rating.isnull(), :]\n",
    "predict_df_1_NA_removed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.1025, MAE: 1.1308, R2: 0.0218\n"
     ]
    }
   ],
   "source": [
    "mse, mae, r2 = mean_squared_error(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating), \\\n",
    "                mean_absolute_error(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating), \\\n",
    "                r2_score(predict_df_1_NA_removed.rating, predict_df_1_NA_removed.pred_rating)\n",
    "print('MSE: {0:.4f}, MAE: {1:.4f}, R2: {2:.4f}'.format(mse, mae, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2: (Implementation please see Appendix)**\n",
    "\n",
    "Build User Profile & Business Profile based on business categories information. As the algorithm takes too long to run, the code is attached in appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Field-Aware Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install CMake\n",
    "# Installation guide https://linxid.github.io/2018/05/10/Xlearn/\n",
    "# !pip install xlearn\n",
    "# !pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30Q5xBagQHmkwp8Q9I1FCg</td>\n",
       "      <td>2018-02-03 23:27:43</td>\n",
       "      <td>5.0</td>\n",
       "      <td>First time at this restaurant our server \"Ramo...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UtWngqS-WloIY_A53W5K-Q</td>\n",
       "      <td>2016-02-18 06:42:16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Such an amazing hospital with friendly staff, ...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76aGN20BrGWvAlYfPGVv_A</td>\n",
       "      <td>2016-04-05 15:25:11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Let me start by saying that I grew up with a f...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FUcwrXBb_ljg3LgTqt6F2g</td>\n",
       "      <td>2018-08-15 22:07:13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love this pharmacy, they compounded a prescrip...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw  2016-11-09 20:09:03     5.0   \n",
       "2  30Q5xBagQHmkwp8Q9I1FCg  2018-02-03 23:27:43     5.0   \n",
       "3  UtWngqS-WloIY_A53W5K-Q  2016-02-18 06:42:16     5.0   \n",
       "5  76aGN20BrGWvAlYfPGVv_A  2016-04-05 15:25:11     5.0   \n",
       "6  FUcwrXBb_ljg3LgTqt6F2g  2018-08-15 22:07:13     5.0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "2  First time at this restaurant our server \"Ramo...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  Such an amazing hospital with friendly staff, ...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "5  Let me start by saying that I grew up with a f...  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "6  Love this pharmacy, they compounded a prescrip...  n6-Gk65cPZL6Uz8qRm3NYw  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "import networkx as nx\n",
    "pd.options.mode.chained_assignment = None\n",
    "import regex as re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'date', 'rating', 'text', 'user_id', 'week_day', 'month',\n",
       "       'hour', 'user_name', 'user_review_count', 'user_yelping_since',\n",
       "       'friends', 'useful_reviews', 'funny_reviews', 'cool_reviews', 'n_fans',\n",
       "       'years_elite', 'average_stars', 'business_name', 'business_address',\n",
       "       'business_city', 'business_state', 'business_latitude',\n",
       "       'business_longitude', 'stars', 'review_counts', 'is_open', 'categories',\n",
       "       'state_avg', 'text_cluster', 'graph_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users_, on = 'user_id')\n",
    "    df = df.merge(business_, on = 'business_id')\n",
    "    rename_dict = {'business_longitude': 'longitude', 'business_latitude': 'latitude',\n",
    "                  'business_state':'state','business_city':'city', 'business_address': 'address'}\n",
    "    df = df.rename(columns = rename_dict)\n",
    "    return df\n",
    "\n",
    "ratings_train = process(ratings_train_.copy())\n",
    "ratings_holdout = process(ratings_holdout_.copy())\n",
    "ratings_val = process(ratings_cv_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_ffm(path, df, type, target, numerics, categories, features, encoder):\n",
    "    # Flagging categorical and numerical fields\n",
    "    print('convert_to_ffm - START')\n",
    "    for x in numerics:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: numeric field - {x}')\n",
    "            encoder['catdict'][x] = 0\n",
    "    for x in categories:\n",
    "        if(x not in encoder['catdict']):\n",
    "            print(f'UPDATING CATDICT: categorical field - {x}')\n",
    "            encoder['catdict'][x] = 1\n",
    "\n",
    "    nrows = df.shape[0]\n",
    "    with open(path + str(type) + \"_ffm.txt\", \"w\") as text_file:\n",
    "\n",
    "        # Looping over rows to convert each row to libffm format\n",
    "        for n, r in enumerate(range(nrows)):\n",
    "            datastring = \"\"\n",
    "            datarow = df.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow[target]))  # Set Target Variable here\n",
    "\n",
    "            # For numerical fields, we are creating a dummy field here\n",
    "            for i, x in enumerate(encoder['catdict'].keys()):\n",
    "                if(encoder['catdict'][x] == 0):\n",
    "                    # Not adding numerical values that are nan\n",
    "                    if math.isnan(datarow[x]) is not True:\n",
    "                        datastring = datastring + \" \"+str(i)+\":\" + str(i)+\":\" + str(datarow[x])\n",
    "                else:\n",
    "\n",
    "                    # For a new field appearing in a training example\n",
    "                    if(x not in encoder['catcodes']):\n",
    "                        print(f'UPDATING CATCODES: categorical field - {x}')\n",
    "                        encoder['catcodes'][x] = {}\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    # For already encoded fields\n",
    "                    elif(datarow[x] not in encoder['catcodes'][x]):\n",
    "                        encoder['currentcode'] += 1\n",
    "                        print(f'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}')\n",
    "                        encoder['catcodes'][x][datarow[x]] = encoder['currentcode']  # encoding the feature\n",
    "\n",
    "                    code = encoder['catcodes'][x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\" + str(int(code))+\":1\"\n",
    "\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Generation \n",
    "\n",
    "In the following steps, different sets of features will be generated as input to FFM.\n",
    "\n",
    "\n",
    "(i) Create user review **text embeddings** based on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_texts = ratings_train.groupby(['user_id'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "rating_texts['text'] = rating_texts['text'].str.lower()\n",
    "rating_texts['text'] = rating_texts['text'].str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words = 'english', strip_accents = 'ascii')\n",
    "vectorizer.fit(rating_texts['text'])\n",
    "vector = vectorizer.transform(rating_texts['text'])\n",
    "tsv = TruncatedSVD(n_components=50)\n",
    "tsv.fit(vector)\n",
    "transformed_tsv = tsv.transform(vector)\n",
    "\n",
    "'''\n",
    "wcss=[]\n",
    "for i in range(77, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(transformed_tsv)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 100), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters=110, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(transformed_tsv)\n",
    "text_cluster = kmeans.predict(transformed_tsv)\n",
    "\n",
    "rating_texts.loc[:,'text_cluster'] = text_cluster\n",
    "rating_texts_features = rating_texts[['user_id','text_cluster']]\n",
    "ratings_train= ratings_train.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')\n",
    "ratings_holdout = ratings_holdout.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')\n",
    "ratings_val = ratings_val.merge(rating_texts_features[['user_id','text_cluster']], on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>week_day</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_counts</th>\n",
       "      <th>is_open</th>\n",
       "      <th>categories</th>\n",
       "      <th>state_avg</th>\n",
       "      <th>text_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>Wilhelmina</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>AZ</td>\n",
       "      <td>33.259702</td>\n",
       "      <td>-111.790203</td>\n",
       "      <td>3.5</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Medical, Cosmetic Dentists, Orthodont...</td>\n",
       "      <td>3.707185</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                date  rating  \\\n",
       "0  WTqjgwHlXbSFevF32_DJVw 2016-11-09 20:09:03     5.0   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  I have to say that this office really has it t...  n6-Gk65cPZL6Uz8qRm3NYw   \n",
       "\n",
       "   week_day  month  hour   user_name  user_review_count     ...       \\\n",
       "0         2     11    20  Wilhelmina                 10     ...        \n",
       "\n",
       "       city state   latitude   longitude  stars  review_counts is_open  \\\n",
       "0  Chandler    AZ  33.259702 -111.790203    3.5             39       1   \n",
       "\n",
       "                                          categories state_avg text_cluster  \n",
       "0  Health & Medical, Cosmetic Dentists, Orthodont...  3.707185           82  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Create **graph** features based on user friends attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = ratings_train[['user_id','friends']].drop_duplicates()\n",
    "train_users_dict = train_users.set_index('user_id').T.to_dict('list')\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_nodes_from(train_users_dict.keys())\n",
    "\n",
    "for k, v in train_users_dict.items():\n",
    "    for i in v[0].split(','):\n",
    "        g.add_edge(k,i.strip())    \n",
    "\n",
    "sub_graphs = nx.connected_component_subgraphs(g)\n",
    "\n",
    "sgs =[]\n",
    "for i, sg in enumerate(sub_graphs):\n",
    "    sgs += [sg]\n",
    "fin_sgs = []\n",
    "for i in sgs:\n",
    "    if len(i.nodes()) >=5:\n",
    "        fin_sgs +=[i]\n",
    "\n",
    "graph_user_ids, graph_ids = [],[]\n",
    "num_id = 0\n",
    "for graph in fin_sgs:\n",
    "    for node in graph.nodes():\n",
    "        graph_user_ids += [node]\n",
    "        graph_ids += [num_id]\n",
    "    num_id += 1\n",
    "social_graphs = pd.DataFrame(\n",
    "    {\"user_id\": graph_user_ids, \"graph_cluster\": graph_ids}\n",
    ")\n",
    "\n",
    "ratings_train= ratings_train.merge(social_graphs, on ='user_id', how = 'left')\n",
    "ratings_holdout = ratings_holdout.merge(social_graphs, on ='user_id', how = 'left')\n",
    "ratings_val = ratings_val.merge(social_graphs, on ='user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(iii) Create **Location** Features: business longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ratings_train[['longitude','latitude']]\n",
    "\n",
    "''' \n",
    "EXPLORATORY ANALYSIS TO DETERMINE NUMBER OF CLUSTERS. DON'T RUN\n",
    "HERE USING THE EBLOW METHOD WE CHOOSE NCLUSTERs OF 10 \n",
    "\n",
    "X = ratings_train[['longitude','latitude']]\n",
    "wcss = []\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 50), wcss[0:48])\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "ratings_train.loc[:,'location_cluster'] = kmeans.predict(ratings_train[['longitude','latitude']])\n",
    "ratings_holdout.loc[:,'location_cluster'] = kmeans.predict(ratings_holdout[['longitude','latitude']])\n",
    "ratings_val.loc[:,'location_cluster'] = kmeans.predict(ratings_val[['longitude','latitude']])\n",
    "\n",
    "ratings_train.text_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.text_cluster.fillna('999', inplace=True)\n",
    "ratings_val.text_cluster.fillna('999', inplace=True)\n",
    "\n",
    "ratings_train.location_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.location_cluster.fillna('999', inplace=True)\n",
    "ratings_val.location_cluster.fillna('999', inplace=True)\n",
    "\n",
    "ratings_train.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_holdout.graph_cluster.fillna('999', inplace=True)\n",
    "ratings_val.graph_cluster.fillna('999', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMModel:\n",
    "\n",
    "    def __init__(self, train, test, config, suffix = None):\n",
    "        self.train_df = train\n",
    "        self.test_df = test\n",
    "        self.model = xl.create_ffm()\n",
    "        self.suffix = suffix\n",
    "        self.config = config\n",
    "        self.preds = None \n",
    "          \n",
    "    def __configure(self):\n",
    "        destination = self.config['destination']\n",
    "        label = self.config['label']\n",
    "        numerical_columns  = self.config['numerical_columns']\n",
    "        categorical_columns  = self.config['categorical_columns']\n",
    "        all_columns  = numerical_columns + categorical_columns\n",
    "              \n",
    "        encoder = {\"currentcode\": len(self.config['numerical_columns']),\n",
    "           \"catdict\": {},\n",
    "           \"catcodes\": {}}\n",
    "        \n",
    "        encoder = _convert_to_ffm(destination, self.train_df , 'train', label, numerical_columns, \\\n",
    "                                  categorical_columns, all_columns, encoder)\n",
    "        encoder = _convert_to_ffm(destination, self.test_df , 'test', label, numerical_columns, \\\n",
    "                                  categorical_columns, all_columns, encoder)\n",
    "        \n",
    "    def train(self, params = None):\n",
    "        encoder = self.__configure()\n",
    "        self.model.setTrain(self.config['destination']+'train_ffm.txt')\n",
    "        self.model.setValidate(self.config['destination']+'test_ffm.txt')\n",
    "        self.model.setTest(self.config['destination']+'test_ffm.txt')\n",
    "        \n",
    "        if not params:\n",
    "            params = {'task': 'reg',\n",
    "                     'lr': 0.2,\n",
    "                     'lambda': 0.002,\n",
    "                     'metric': 'auc'}\n",
    "        \n",
    "        self.model.fit(params, self.config['model_destination']+self.config['model_name']+'.out')\n",
    "\n",
    "    def evaluate_on_val(self):\n",
    "        self.model.predict(self.config['model_destination'] + self.config['model_name']+'.out', \\\n",
    "                           self.config['output_destination']+'predictions.txt')\n",
    "        preds = pd.read_csv(self.config['output_destination']+'predictions.txt', sep=\" \", header=None)\n",
    "        preds.columns = ['prediction']\n",
    "        preds.prediction = np.clip(preds.prediction,0.0,5.0)\n",
    "        self.preds = preds\n",
    "        return preds            \n",
    "    \n",
    "    def get_regression_metrics(self):\n",
    "        if self.preds is None:\n",
    "            self.evaluate_on_val()\n",
    "        \n",
    "        predictions = self.evaluate_on_val()\n",
    "        test_df = self.test_df.copy()\n",
    "        test_df['preds']  = np.clip(predictions.values,0.0,5.0)\n",
    "        \n",
    "        r2 = r2_score(test_df['rating'], test_df['preds'])\n",
    "        mse = mean_squared_error(test_df['rating'], test_df['preds'])\n",
    "        mae = mean_absolute_error(test_df['rating'], test_df['preds'])\n",
    "        \n",
    "        print('R2: ', r2, ' MSE: ',mse, ' MAE: ', mae)\n",
    "        return r2,mse,mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting and Evaluation\n",
    "\n",
    "Now run FFM on different combinations of features and calculate R2, mean squared error (MSE) and mean absolute error (MAE) on them.\n",
    "\n",
    "#### Metrics with just userid and business id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'baseline',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_FFM = FFMModel(ratings_train, ratings_val, baseline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "baseline_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1873866119640475  MSE:  1.7830047851238464  MAE:  1.0886505079082125\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = baseline_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with Baseline + Business Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'user_business',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_business_FFM = FFMModel(ratings_train, ratings_val, user_business_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "user_business_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1964585848489021  MSE:  1.763099414005971  MAE:  1.0615091977315227\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = user_business_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with Baseline + Location Cluster (has best MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','location_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'location',\n",
    "    'output_destination': 'output/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_FFM = FFMModel(ratings_train, ratings_val, location_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "location_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.19744277000305088  MSE:  1.760939953104722  MAE:  1.0666155431063107\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = location_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with User and Business Information + Text Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','text_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'text',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "text_FFM = FFMModel(ratings_train, ratings_val, text_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "text_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.18948085767652612  MSE:  1.778409672390595  MAE:  1.064447769604502\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = text_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics with User and Business Information + Graph Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','city','state'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'graph',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "graph_FFM = FFMModel(ratings_train, ratings_val, graph_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "graph_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.19572440525321522  MSE:  1.7647103224053686  MAE:  1.072474083328964\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = graph_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics All clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_config = {   'destination': 'data/',\n",
    "    'categorical_columns':['business_id','user_id','graph_cluster','location_cluster','text_cluster'],\n",
    "    'numerical_columns' : [],\n",
    "    'label' : 'rating',\n",
    "    'model_destination' : 'trained_models/',\n",
    "    'model_name' : 'all_clusters',\n",
    "    'output_destination': 'output/'\n",
    "}\n",
    "all_FFM = FFMModel(ratings_train, ratings_val, all_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "all_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2:  0.1862330235045122  MSE:  1.7855359441887817  MAE:  1.065033877918174\n"
     ]
    }
   ],
   "source": [
    "r2, mse, mae = all_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The baseline with location cluster performs best as it has lowest MSE. Therefore, this model will be used. The following analysis is done on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM runs in linear time which leads to fast runtime. Our model takes just two minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import time\n",
    "start_time = time.time()\n",
    "location_FFM = FFMModel(ratings_train, ratings_val, location_config)\n",
    "location_FFM.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 206.21558594703674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "start_time = time.time()\n",
    "location_FFM.get_regression_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.16828489303588867 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage and Serendipity metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = location_FFM.evaluate_on_val()\n",
    "predict_df['predictions'] = predictions['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011627906976744186 0.8576470588235293 0.9990697674418605 72.29767441860466\n"
     ]
    }
   ],
   "source": [
    "top_10, coverage, serendipity, avg_rank = get_all_metrics(predict_df, out, ratings_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepctr[cpu]\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/46/2a3291f804c56ba0e5ba31d6c825c23bb1a2c7d8f6f066db28f8ed5f374a/deepctr-0.7.0-py3-none-any.whl (79kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting requests (from deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.9MB 418kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\" in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from deepctr[cpu])\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from requests->deepctr[cpu])\n",
      "Collecting idna<2.9,>=2.5 (from requests->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 10.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting six (from h5py->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.7 (from h5py->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.0MB 60kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: keras-preprocessing>=1.0.5 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: termcolor>=1.1.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: tensorboard<1.15.0,>=1.14.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: keras-applications>=1.0.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: protobuf>=3.6.1 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: google-pasta>=0.1.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: grpcio>=1.8.6 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: absl-py>=0.7.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: gast>=0.2.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: astor>=0.6.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Collecting wheel>=0.26 (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: wrapt>=1.11.1 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 3.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: setuptools>=41.0.0 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Requirement already up-to-date: markdown>=2.6.8 in /home/jzljohn18/anaconda3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow!=1.7.*,!=1.8.*,>=1.4.0; extra == \"cpu\"->deepctr[cpu])\n",
      "Installing collected packages: urllib3, certifi, idna, requests, six, numpy, h5py, deepctr, wheel, werkzeug\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Found existing installation: certifi 2018.1.18\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (certifi) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling certifi-2018.1.18:\n",
      "      Successfully uninstalled certifi-2018.1.18\n",
      "  Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: six 1.11.0\n",
      "    Uninstalling six-1.11.0:\n",
      "      Successfully uninstalled six-1.11.0\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: h5py 2.7.1\n",
      "    Uninstalling h5py-2.7.1:\n",
      "      Successfully uninstalled h5py-2.7.1\n",
      "  Found existing installation: wheel 0.30.0\n",
      "    Uninstalling wheel-0.30.0:\n",
      "      Successfully uninstalled wheel-0.30.0\n",
      "  Found existing installation: Werkzeug 0.14.1\n",
      "    Uninstalling Werkzeug-0.14.1:\n",
      "      Successfully uninstalled Werkzeug-0.14.1\n",
      "Successfully installed certifi-2019.11.28 deepctr-0.7.0 h5py-2.10.0 idna-2.8 numpy-1.17.4 requests-2.22.0 six-1.13.0 urllib3-1.25.7 werkzeug-0.16.0 wheel-0.33.6\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U deepctr[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7a0PQGQjwC1B"
   },
   "source": [
    "## Deep Learning Models\n",
    "#### DeepFM\n",
    "\n",
    "![DeepFM Model Architecture](https://d2l.ai/_images/rec-deepfm.svg)\n",
    "\n",
    "It is a model which integrates the feature representation learning of a neural network with factorization machines.\n",
    "\n",
    "Adding nonlinear transformation layers to factorization machines gives it the capability to model both low-order feature combinations and high-order feature combinations. Moreover, non-linear inherent structures from inputs can also be captured with neural networks.\n",
    "\n",
    "#### Wide and Deep Learning\n",
    "\n",
    "![Wide and Deep Model](https://2.bp.blogspot.com/-wkrmRibw_GM/V3Mg3O3Q0-I/AAAAAAAABG0/Jm3Nl4-VcYIJ44dA5nSz6vpTyCKF2KWQgCKgB/s640/image03.png)\n",
    "\n",
    "The Wide part of the model tries to capture the co-occurrence of a query-item feature pair correlates with the target label. The Deep model generalizes the query-item interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NW95ttnU-3ed",
    "outputId": "c1101d84-e388-4386-91c7-b78bf7061f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')#, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "OSPql3L9d3qx",
    "outputId": "ebe6f8eb-5d36-4fd6-818b-a349394bf8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: deepctr[gpu] in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\" in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr[gpu]) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->deepctr[gpu]) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (42.0.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.8)\n",
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U deepctr[gpu]\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4lubeylyJLF"
   },
   "source": [
    "The GPU being used for the deep learning models is a Tesla P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "RIgViUOV9WQl",
    "outputId": "bd1e1569-1c61-4881-ce3b-7ea280d72281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 20 14:14:03 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1j6rJwyyOzN"
   },
   "source": [
    "#### Importing packages and reading the dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pICvjmggcnoI"
   },
   "outputs": [],
   "source": [
    "## data handling\n",
    "# setup libraries and env\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.models import DeepFM, CCPM, FNN, PNN, WDL, MLR, NFM, AFM, DCN, DIN, DIEN, DSIN, xDeepFM, AutoInt, NFFM, FGCNN, FiBiNET\n",
    "from deepctr.inputs import SparseFeat,get_feature_names\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "training =  pd.read_csv('/content/drive/My Drive/final_project_datasets/ratings_sample_train_20.csv', index_col = 0)\n",
    "validation = pd.read_csv('/content/drive/My Drive/final_project_datasets/ratings_sample_validation_20.csv', index_col = 0)\n",
    "test = pd.read_csv('/content/drive/My Drive/final_project_datasets/ratings_sample_test_20.csv', index_col = 0)\n",
    "\n",
    "businesses = pd.read_csv('/content/drive/My Drive/final_project_datasets/businesses.csv')\n",
    "users = pd.read_csv('/content/drive/My Drive/final_project_datasets/active_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "slD0osLPVmN1"
   },
   "outputs": [],
   "source": [
    "# training.dropna(inplace = True)\n",
    "# validation.dropna(inplace = True)\n",
    "# test.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jajtKWzSySTR"
   },
   "source": [
    "##### Include cities as a feature in the deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fFyOfYm_Stb"
   },
   "outputs": [],
   "source": [
    "businesses['business_city_state'] = businesses['business_city'] + businesses['business_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W90pYEhiVCxo",
    "outputId": "7e04e41c-8738-4f5d-dce0-f9bed277781b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(803897, 5) (57229, 5) (57223, 5)\n"
     ]
    }
   ],
   "source": [
    "print(training.shape, validation.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58mUcfi7BFyY"
   },
   "outputs": [],
   "source": [
    "training = training.merge(right = businesses[['business_id', 'business_city_state']], how = 'left', on = 'business_id')\n",
    "validation = validation.merge(right = businesses[['business_id', 'business_city_state']], how = 'left', on = 'business_id')\n",
    "test = test.merge(right = businesses[['business_id', 'business_city_state']], how = 'left', on = 'business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJmXuQftBuz9"
   },
   "outputs": [],
   "source": [
    "column_sequence = ['user_id', 'business_id', 'business_city_state', 'text', 'rating', 'date']\n",
    "training = training[column_sequence]\n",
    "validation = validation[column_sequence]\n",
    "test = test[column_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Yk_DkOyO8A3V",
    "outputId": "585e9993-05e4-437c-a8d9-e36777c28f66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_city_state</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "      <td>hk5wpV-_pi5jmDDVPeG8DA</td>\n",
       "      <td>MesaAZ</td>\n",
       "      <td>I highly recommend Arizona Pet Mortuary, David...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-09-14 18:50:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "      <td>qdCwzhJ5Yo_Sdm_bYDIfOQ</td>\n",
       "      <td>AhwatukeeAZ</td>\n",
       "      <td>I found Kathy's from yelp.  I love to support ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-09-11 06:09:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "      <td>XS1Zx6GzjtKPKmhDuVw5Jg</td>\n",
       "      <td>ClevelandOH</td>\n",
       "      <td>I had the Saison infused with grapefruit which...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017-06-19 22:55:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "      <td>jLxeBgWhLRbII2ACkgH1Sg</td>\n",
       "      <td>Las VegasNV</td>\n",
       "      <td>First time for me to come inside at least! Hav...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-30 18:00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TpyOT5E16YASd7EWjLQlrw</td>\n",
       "      <td>U_yacPCk8HgE1ywATmQUrg</td>\n",
       "      <td>EtobicokeON</td>\n",
       "      <td>Ordered for lunch with a few colleagues throug...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-10-13 00:10:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  ... rating                 date\n",
       "0  n6-Gk65cPZL6Uz8qRm3NYw  hk5wpV-_pi5jmDDVPeG8DA  ...    5.0  2018-09-14 18:50:19\n",
       "1  d6xvYpyzcfbF_AZ8vMB7QA  qdCwzhJ5Yo_Sdm_bYDIfOQ  ...    2.0  2011-09-11 06:09:33\n",
       "2  sG_h0dIzTKWa3Q6fmb4u-g  XS1Zx6GzjtKPKmhDuVw5Jg  ...    3.0  2017-06-19 22:55:06\n",
       "3  FIk4lQQu1eTe2EpzQ4xhBA  jLxeBgWhLRbII2ACkgH1Sg  ...    4.0  2018-09-30 18:00:41\n",
       "4  TpyOT5E16YASd7EWjLQlrw  U_yacPCk8HgE1ywATmQUrg  ...    5.0  2018-10-13 00:10:59\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert object to datetime\n",
    "# training.date = pd.to_datetime(training.date)\n",
    "# validation.date = pd.to_datetime(validation.date)\n",
    "# test.date = pd.to_datetime(test.date)\n",
    "\n",
    "# find hour from datetime\n",
    "# training['hour'] = training.date.dt.hour\n",
    "# validation['hour'] = validation.date.dt.hour\n",
    "# test['hour'] = test.date.dt.hour\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IXUwUgTyb4R"
   },
   "source": [
    "##### Data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v0viHy3k8EFt",
    "outputId": "a046d41a-1963-48b9-8786-9421320bf645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# quality check\n",
    "print(len(set(test.user_id) - set(training.user_id)))\n",
    "print(len(set(validation.user_id) - set(training.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BetkE-yUgym-"
   },
   "outputs": [],
   "source": [
    "test = test.loc[test.business_id.isin(training.business_id)]\n",
    "validation = validation.loc[validation.business_id.isin(training.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vkFO-bV4rJb"
   },
   "outputs": [],
   "source": [
    "# map each user_id, business_id to an index\n",
    "# user_mapping = {}\n",
    "# for n,i in enumerate(training.user_id.unique()):\n",
    "#   user_mapping[i] = n\n",
    "\n",
    "# business_mapping = {}\n",
    "# for n,i in enumerate(training.business_id.unique()):\n",
    "#   business_mapping[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwqf8DZ66Z8r"
   },
   "outputs": [],
   "source": [
    "# for training\n",
    "# training['user_id'] = training['user_id'].map(user_mapping)\n",
    "# training['business_id'] = training['business_id'].map(business_mapping)\n",
    "# for validation\n",
    "# validation['user_id'] = validation['user_id'].map(user_mapping)\n",
    "# validation['business_id'] = validation['business_id'].map(business_mapping)\n",
    "# for test\n",
    "# test['user_id'] = test['user_id'].map(user_mapping)\n",
    "# test['business_id'] = test['business_id'].map(business_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZCTJonRY6j7s",
    "outputId": "89db0376-12ac-4354-ae9e-6973e9ebb804"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_city_state</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d6xvYpyzcfbF_AZ8vMB7QA</td>\n",
       "      <td>qdCwzhJ5Yo_Sdm_bYDIfOQ</td>\n",
       "      <td>AhwatukeeAZ</td>\n",
       "      <td>I found Kathy's from yelp.  I love to support ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-09-11 06:09:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sG_h0dIzTKWa3Q6fmb4u-g</td>\n",
       "      <td>XS1Zx6GzjtKPKmhDuVw5Jg</td>\n",
       "      <td>ClevelandOH</td>\n",
       "      <td>I had the Saison infused with grapefruit which...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017-06-19 22:55:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "      <td>jLxeBgWhLRbII2ACkgH1Sg</td>\n",
       "      <td>Las VegasNV</td>\n",
       "      <td>First time for me to come inside at least! Hav...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018-09-30 18:00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TpyOT5E16YASd7EWjLQlrw</td>\n",
       "      <td>U_yacPCk8HgE1ywATmQUrg</td>\n",
       "      <td>EtobicokeON</td>\n",
       "      <td>Ordered for lunch with a few colleagues throug...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-10-13 00:10:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_N7Ndn29bpll_961oPeEfw</td>\n",
       "      <td>O-b5osM0NO4f31dp6_DatQ</td>\n",
       "      <td>TorontoON</td>\n",
       "      <td>I can only comment on their macarons, which I'...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-08-01 01:55:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  ... rating                 date\n",
       "1  d6xvYpyzcfbF_AZ8vMB7QA  qdCwzhJ5Yo_Sdm_bYDIfOQ  ...    2.0  2011-09-11 06:09:33\n",
       "2  sG_h0dIzTKWa3Q6fmb4u-g  XS1Zx6GzjtKPKmhDuVw5Jg  ...    3.0  2017-06-19 22:55:06\n",
       "3  FIk4lQQu1eTe2EpzQ4xhBA  jLxeBgWhLRbII2ACkgH1Sg  ...    4.0  2018-09-30 18:00:41\n",
       "4  TpyOT5E16YASd7EWjLQlrw  U_yacPCk8HgE1ywATmQUrg  ...    5.0  2018-10-13 00:10:59\n",
       "5  _N7Ndn29bpll_961oPeEfw  O-b5osM0NO4f31dp6_DatQ  ...    3.0  2014-08-01 01:55:23\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAW_-RGWlge5"
   },
   "outputs": [],
   "source": [
    "# validation = validation.loc[~validation.business_id.isin(['WpC53SqwoCY5AuYIFr_1eA'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9rQWXdEyhPj"
   },
   "source": [
    "#### Preparation for input into deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPZ-2X6hj_e6"
   },
   "outputs": [],
   "source": [
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "training = training.loc[training.business_city_state.apply(type) != float]\n",
    "training_deep = training.copy()\n",
    "validation_deep = validation.copy()\n",
    "test_deep = test.copy()\n",
    "\n",
    "sparse_features = [\"user_id\", \"business_id\", \"business_city_state\"]#, \"hour\"]\n",
    "target = ['rating']\n",
    "for feat in sparse_features:\n",
    "  lbe = LabelEncoder()\n",
    "  training_deep[feat] = lbe.fit_transform(training_deep[feat])\n",
    "  validation_deep[feat] = lbe.transform(validation_deep[feat])\n",
    "  test_deep[feat] = lbe.transform(test_deep[feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EneZP-byvEy"
   },
   "source": [
    "##### Grid Search - Hyperparameter Tuning\n",
    "\n",
    "We have tuned 3 parameters for both the models,\n",
    "1. Embedding dimension - 8, 16, 32\n",
    "2. Hidden Units - (128, 128), (256, 256), (256, 128)\n",
    "3. Dropout - 0.1, 0.3, 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B61Aov2Hs3r6"
   },
   "source": [
    "#### Grid Search for DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "c73rkdvjI70c",
    "outputId": "66a24171-9da1-4763-f396-eefebb3e7b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 803897 samples, validate on 53225 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803897/803897 - 70s - loss: 1.7237 - mse: 1.7062 - val_loss: 1.7827 - val_mse: 1.7481\n",
      "validation MSE 1.7481\n",
      "Train on 803897 samples, validate on 53225 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'embedding_dim' : [8, 16, 32],\n",
    "    'dnn_hidden_units': [(128, 128), (256, 256), (256, 128)],\n",
    "    'dnn_dropout': [0.1, 0.3, 0.5]\n",
    "}\n",
    "allNames = sorted(params)\n",
    "combinations = it.product(*(params[Name] for Name in allNames))\n",
    "\n",
    "best_params = None\n",
    "best_mse = 1000\n",
    "\n",
    "# grid search for DeepFM\n",
    "for i in list(combinations):\n",
    "  dropout, dnn_hidden_units, embedding_dim = i\n",
    "\n",
    "  # 2.count #unique features for each sparse field\n",
    "  fixlen_feature_columns = [SparseFeat(feat, training_deep[feat].nunique(), embedding_dim = embedding_dim) for feat in sparse_features]\n",
    "  linear_feature_columns = fixlen_feature_columns\n",
    "  dnn_feature_columns = fixlen_feature_columns\n",
    "  feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "  \n",
    "  # 3.define model inputs   \n",
    "  train_model_input = {name:training_deep[name].values for name in feature_names}\n",
    "  valid_model_input = {name:validation_deep[name].values for name in feature_names}\n",
    "  test_model_input = {name:test_deep[name].values for name in feature_names}\n",
    "\n",
    "  # 4.Define Model,train,predict and evaluate\n",
    "  model = DeepFM(linear_feature_columns, dnn_feature_columns, fm_group=['business_city_state'], task='regression', dnn_hidden_units = dnn_hidden_units, dnn_dropout = dropout, l2_reg_embedding=1e-05, l2_reg_dnn=1e-05, l2_reg_linear=1e-05, seed = 42)\n",
    "  model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "  # only one epoch because it overfits after the first epoch\n",
    "  history = model.fit(train_model_input, training_deep[target].values, batch_size=256, epochs=1, verbose=2, validation_data= (valid_model_input, validation[target].values))\n",
    "\n",
    "  validation_predictions = model.predict(valid_model_input, batch_size=256)\n",
    "  val_mse = mean_squared_error(validation_deep[target].values, validation_predictions)\n",
    "  print(\"validation MSE\", round(val_mse, 4))\n",
    "  if val_mse < best_mse:\n",
    "    best_mse = val_mse\n",
    "    best_params = [dropout, dnn_hidden_units, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRBnLR47PGW4"
   },
   "outputs": [],
   "source": [
    "best_mse, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Cqr0e7ds9Ga"
   },
   "source": [
    "#### Grid Search for WDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIIAcVo4PSON"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embedding_dim' : [8, 16, 32],\n",
    "    'dnn_hidden_units': [(128, 128), (256, 256), (256, 128)],\n",
    "    'dnn_dropout': [0.1, 0.3, 0.5]\n",
    "}\n",
    "allNames = sorted(params)\n",
    "combinations = it.product(*(params[Name] for Name in allNames))\n",
    "\n",
    "best_params_wdl = None\n",
    "best_mse_wdl = 1000\n",
    "\n",
    "# grid search for WDL\n",
    "for i in list(combinations):\n",
    "  dropout, dnn_hidden_units, embedding_dim = i\n",
    "\n",
    "  # 2.count #unique features for each sparse field\n",
    "  fixlen_feature_columns = [SparseFeat(feat, training_deep[feat].nunique(), embedding_dim = embedding_dim) for feat in sparse_features]\n",
    "  linear_feature_columns = fixlen_feature_columns\n",
    "  dnn_feature_columns = fixlen_feature_columns\n",
    "  feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "  \n",
    "  # 3.define model inputs   \n",
    "  train_model_input = {name:training_deep[name].values for name in feature_names}\n",
    "  valid_model_input = {name:validation_deep[name].values for name in feature_names}\n",
    "  test_model_input = {name:test_deep[name].values for name in feature_names}\n",
    "\n",
    "  # 4.Define Model,train,predict and evaluate\n",
    "  model = WDL(linear_feature_columns, dnn_feature_columns, task='regression', dnn_hidden_units = dnn_hidden_units, dnn_dropout = dropout, l2_reg_embedding=1e-05, l2_reg_dnn=1e-05, l2_reg_linear=1e-05, seed = 42)\n",
    "  model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "  # only one epoch because it overfits after the first epoch\n",
    "  history = model.fit(train_model_input, training_deep[target].values, batch_size=256, epochs=1, verbose=2, validation_data= (valid_model_input, validation[target].values))\n",
    "\n",
    "  validation_predictions = model.predict(valid_model_input, batch_size=256)\n",
    "  val_mse = mean_squared_error(validation_deep[target].values, validation_predictions)\n",
    "  print(\"validation MSE\", round(val_mse, 4))\n",
    "  if val_mse < best_mse_wdl:\n",
    "    best_mse_wdl = val_mse\n",
    "    best_params_wdl = [dropout, dnn_hidden_units, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uXv2eFIPSL8"
   },
   "outputs": [],
   "source": [
    "best_mse_wdl, best_params_wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-iSvuv1zGS3"
   },
   "source": [
    "#### Refitting the model on the best parameters for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "4YwMQt-8pIPT",
    "outputId": "2f25c438-8bab-4fdd-cc65-8ee7c9f007e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 857122 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857122/857122 - 72s - loss: 1.7143 - mse: 1.6955\n",
      "Train on 857122 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857122/857122 - 71s - loss: 1.7126 - mse: 1.6932\n"
     ]
    }
   ],
   "source": [
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "training_combined = pd.concat([training, validation], axis = 0)\n",
    "training_combined_deep = training_combined.copy()\n",
    "test_deep = test.copy()\n",
    "\n",
    "sparse_features = [\"user_id\", \"business_id\", \"business_city_state\"]#, \"hour\"]\n",
    "target = ['rating']\n",
    "\n",
    "for feat in sparse_features:\n",
    "  lbe = LabelEncoder()\n",
    "  training_combined_deep[feat] = lbe.fit_transform(training_combined_deep[feat])\n",
    "  test_deep[feat] = lbe.transform(test_deep[feat])\n",
    "\n",
    "# best DeepFM Model\n",
    "dropout_deepfm = 0.1\n",
    "dnn_hidden_units_deepfm = (128, 128)\n",
    "embedding_dim_deepfm = 8\n",
    "\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, training_combined_deep[feat].nunique(), embedding_dim = embedding_dim_deepfm) for feat in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "# 3.define model inputs   \n",
    "train_model_input = {name:training_combined_deep[name].values for name in feature_names}\n",
    "test_model_input = {name:test_deep[name].values for name in feature_names}\n",
    "\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "model_deepfm = DeepFM(linear_feature_columns, dnn_feature_columns, fm_group=['business_city_state'], task='regression', dnn_hidden_units = dnn_hidden_units_deepfm, dnn_dropout = dropout_deepfm, l2_reg_embedding=1e-05, l2_reg_dnn=1e-05, l2_reg_linear=1e-05, seed = 42)\n",
    "model_deepfm.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "# only one epoch because it overfits after the first epoch\n",
    "history = model_deepfm.fit(train_model_input, training_combined_deep[target].values, batch_size=256, epochs=1, verbose=2)\n",
    "\n",
    "test_predictions_deepfm = model_deepfm.predict(test_model_input, batch_size=255)\n",
    "test_mse_deepfm = mean_squared_error(test_deep[target].values, test_predictions_deepfm)\n",
    "\n",
    "\n",
    "# best WDL Model\n",
    "dropout_wdl = 0.3\n",
    "dnn_hidden_units_wdl = (256, 256)\n",
    "embedding_dim_wdl = 8\n",
    "\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, training_combined_deep[feat].nunique(), embedding_dim = embedding_dim_wdl) for feat in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "# 3.define model inputs   \n",
    "train_model_input = {name:training_combined_deep[name].values for name in feature_names}\n",
    "test_model_input = {name:test_deep[name].values for name in feature_names}\n",
    "\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "model_wdl = WDL(linear_feature_columns, dnn_feature_columns, task='regression', dnn_hidden_units = dnn_hidden_units_wdl, dnn_dropout = dropout_wdl, l2_reg_embedding=1e-05, l2_reg_dnn=1e-05, l2_reg_linear=1e-05, seed = 42)\n",
    "model_wdl.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "# only one epoch because it overfits after the first epoch\n",
    "history = model_wdl.fit(train_model_input, training_combined_deep[target].values, batch_size=256, epochs=1, verbose=2)\n",
    "\n",
    "test_predictions_wdl = model_wdl.predict(test_model_input, batch_size=256)\n",
    "test_mse_wdl = mean_squared_error(test_deep[target].values, test_predictions_wdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m9r7HJH_u_qc",
    "outputId": "16921fe0-56a2-4d8e-82ee-bbd0e1177041"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7833152958561953, 1.784929756718987)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mse_deepfm, test_mse_wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ntPEspLzQfk"
   },
   "source": [
    "#### R2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3gxa_XCvQHPO",
    "outputId": "efbf621b-9687-4fa8-d077-e2e6687233e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.204\n",
      "R2 Score: 0.203\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 Score: %0.3f\" %r2_score(y_true = test_deep[target], y_pred = test_predictions_deepfm))\n",
    "print(\"R2 Score: %0.3f\" %r2_score(y_true = test_deep[target], y_pred = test_predictions_wdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAPdjZHTzT9L"
   },
   "source": [
    "#### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eXVSUJR1FtIQ",
    "outputId": "a9756783-66e1-48fe-95d1-d406644d251f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.082\n",
      "Mean Absolute Error: 1.079\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Absolute Error: %0.3f\" %mean_absolute_error(y_true = test_deep[target], y_pred = test_predictions_deepfm))\n",
    "print(\"Mean Absolute Error: %0.3f\" %mean_absolute_error(y_true = test_deep[target], y_pred = test_predictions_wdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vq62aTXfzWKX"
   },
   "source": [
    "#### Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Mx18EHNdFtsy",
    "outputId": "defa932b-6f58-4169-d68a-df4eb9c20eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error: 1.335\n",
      "Root Mean Square Error: 1.336\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Square Error: %0.3f\" %mean_squared_error(y_true = test_deep[target], y_pred = test_predictions_deepfm, squared = False))\n",
    "print(\"Root Mean Square Error: %0.3f\" %mean_squared_error(y_true = test_deep[target], y_pred = test_predictions_wdl, squared = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HviC2U69-cvT"
   },
   "source": [
    "#### Rest of the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJaZIbgdFtx4"
   },
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    # df = df.drop(df.columns[0], axis =1)\n",
    "    df['date']  = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df = df.merge(users, on = 'user_id')\n",
    "    df = df.merge(businesses, on = 'business_id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-AgdF0vOFtwd"
   },
   "outputs": [],
   "source": [
    "ratings_train = process(training.copy())\n",
    "ratings_validation = process(validation.copy())\n",
    "ratings_test = process(test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44J-_bemFtqn"
   },
   "outputs": [],
   "source": [
    "ratings_train_final = ratings_train.append(ratings_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3wqepVHQC54"
   },
   "outputs": [],
   "source": [
    "unique_city_businesses = ratings_train_final[['business_city','business_id']].drop_duplicates()\n",
    "unique_cities = unique_city_businesses.groupby('business_city').count()['business_id']\n",
    "unique_cities = unique_cities[unique_cities > 100]\n",
    "out = pd.DataFrame()\n",
    "for city in unique_cities.index:\n",
    "    tmp = ratings_train_final[(ratings_train_final['business_city'] ==city) &\n",
    "                              (ratings_train_final['rating'] >ratings_train_final['average_stars'])]\n",
    "    if len(tmp['user_id'].unique())>4:\n",
    "        np.random.seed(42)\n",
    "        ###this weird sampling technique is to ensure we dont' sample the same user twice in a same city\n",
    "        five_users = np.random.choice(tmp['user_id'].unique(),5, replace = False)\n",
    "        row = tmp[tmp['user_id'].isin(five_users)].groupby('user_id', group_keys=False).apply(lambda df: df.sample(1))\n",
    "        out = out.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XbttGKw7i4OF",
    "outputId": "f7458a6c-0035-4f81-84c7-fd0f79969f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(out.groupby('business_city').count()['user_id']==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJ2MUDxni4TG"
   },
   "outputs": [],
   "source": [
    "predict_df = out[['user_id','business_city','business_state']]\n",
    "predict_df = predict_df.merge(unique_city_businesses, on = 'business_city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2rWcE5DHi4Ri",
    "outputId": "d1ff79de-df74-45f8-9aac-2e525476cc36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(predict_df.groupby('business_city')['user_id'].nunique()==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIB3gLs1mzaS"
   },
   "outputs": [],
   "source": [
    "# remove businesses not in training\n",
    "predict_df = predict_df.loc[predict_df.business_id.isin(training.business_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "87I2AzH2i4Lp",
    "outputId": "9b779b9c-63f1-47f3-8f87-7829c113cd4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>8AW0koYMDa1PlJMOE-b2-g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>-YGQwikbX2fXUIjyegR7pw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>5Kh5i4VhXj-Leg8gujIzjQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>Wl1oOVbtK4I9vRKoaSKYiQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>OxSaGGTmIujsjDpDqwyGPQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567480</th>\n",
       "      <td>gWbXQg0rPLDCRNR0HbImvA</td>\n",
       "      <td>nkLUGjzFNPCrClbT1UIZaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567481</th>\n",
       "      <td>gWbXQg0rPLDCRNR0HbImvA</td>\n",
       "      <td>r0U1aexkjoUKoTuXlisjng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567482</th>\n",
       "      <td>gWbXQg0rPLDCRNR0HbImvA</td>\n",
       "      <td>KfuHr7dYyEaDrHtQpFdNUw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567483</th>\n",
       "      <td>gWbXQg0rPLDCRNR0HbImvA</td>\n",
       "      <td>41xuKlIuZTLu6qTbpqTY-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567484</th>\n",
       "      <td>gWbXQg0rPLDCRNR0HbImvA</td>\n",
       "      <td>PWSMRc9FW9fbUa1WJ0mhDQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567485 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id\n",
       "0       7kfJk_NtOslC9jPk2Koz3g  8AW0koYMDa1PlJMOE-b2-g\n",
       "1       7kfJk_NtOslC9jPk2Koz3g  -YGQwikbX2fXUIjyegR7pw\n",
       "2       7kfJk_NtOslC9jPk2Koz3g  5Kh5i4VhXj-Leg8gujIzjQ\n",
       "3       7kfJk_NtOslC9jPk2Koz3g  Wl1oOVbtK4I9vRKoaSKYiQ\n",
       "4       7kfJk_NtOslC9jPk2Koz3g  OxSaGGTmIujsjDpDqwyGPQ\n",
       "...                        ...                     ...\n",
       "567480  gWbXQg0rPLDCRNR0HbImvA  nkLUGjzFNPCrClbT1UIZaw\n",
       "567481  gWbXQg0rPLDCRNR0HbImvA  r0U1aexkjoUKoTuXlisjng\n",
       "567482  gWbXQg0rPLDCRNR0HbImvA  KfuHr7dYyEaDrHtQpFdNUw\n",
       "567483  gWbXQg0rPLDCRNR0HbImvA  41xuKlIuZTLu6qTbpqTY-A\n",
       "567484  gWbXQg0rPLDCRNR0HbImvA  PWSMRc9FW9fbUa1WJ0mhDQ\n",
       "\n",
       "[567485 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df[['user_id', 'business_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_osHNri4KK"
   },
   "outputs": [],
   "source": [
    "predict_df['business_city_state'] = predict_df['business_city'] + predict_df['business_state']\n",
    "metric_test_deep = predict_df[['user_id', 'business_id', 'business_city_state']].copy()\n",
    "\n",
    "for feat in sparse_features:\n",
    "  lbe = LabelEncoder()\n",
    "  lbe.fit(training[feat])\n",
    "  metric_test_deep[feat] = lbe.transform(metric_test_deep[feat])\n",
    "  \n",
    "metric_test_input = {name:metric_test_deep[name].values for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIGGwTB5i4Ev"
   },
   "outputs": [],
   "source": [
    "metric_test_predictions_deepfm = model_deepfm.predict(metric_test_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVpeNfgFBAJO"
   },
   "outputs": [],
   "source": [
    "metric_test_predictions_wdl = model_wdl.predict(metric_test_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_y1tyX2qnci"
   },
   "outputs": [],
   "source": [
    "predict_df['predictions'] = metric_test_predictions_wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FyT-eH8Zi4CD"
   },
   "outputs": [],
   "source": [
    "top_10_recs = predict_df.groupby(['user_id','business_city'])['predictions'].nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c-OQeKTKqu0c",
    "outputId": "2edfcf56-5289-44e1-8613-57009001eac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(top_10_recs.groupby('business_city')['user_id'].count()==50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSLYCDXIqxxz"
   },
   "outputs": [],
   "source": [
    "cnt =0\n",
    "serendipity = 0\n",
    "for row in out.iterrows():\n",
    "    row_values = row[1]\n",
    "    top_10 = predict_df.loc[top_10_recs[top_10_recs['user_id'] == row_values['user_id']].level_2]['business_id']\n",
    "    ###In top 10\n",
    "    if row_values['business_id'] in top_10.values:\n",
    "        cnt+=1\n",
    "    user_history = ratings_train_final[ratings_train_final['user_id'] == row_values['user_id']]    \n",
    "    been_there = [i for i in top_10.values if i in  user_history.business_id.values]\n",
    "    serendipity += 1-len(been_there)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ylHkaMQzpo0"
   },
   "source": [
    "##### Inclusion of Last Review in Top 10 Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3qnqgccSqz8N",
    "outputId": "1a0815da-26af-4de4-a340-5c49a3c60269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15421686746987953"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt/len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-npOsGnLzylw"
   },
   "source": [
    "#### Novelty(% of new restaurants in top 10 recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nLVvzmSgq0F3",
    "outputId": "935e64f8-eee4-4b77-ebb8-a57c396c769e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710843373493965"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serendipity/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdU5aij8q0Jz"
   },
   "outputs": [],
   "source": [
    "predict_df = predict_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "OhWnlJlRq0D0",
    "outputId": "4fe44004-326d-4071-bdea-f512b496425c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'user_id', 'business_city', 'business_state', 'business_id',\n",
       "       'business_city_state', 'predictions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "d8NoYo-_q0BZ",
    "outputId": "4d290edc-7111-43b1-d554-a16b1658e109"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_city</th>\n",
       "      <th>level_2</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--3WaS23LcIXtxyFULJHTA</td>\n",
       "      <td>Scottsdale</td>\n",
       "      <td>442142</td>\n",
       "      <td>5.031378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id business_city  level_2  predictions\n",
       "0  --3WaS23LcIXtxyFULJHTA    Scottsdale   442142     5.031378"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_recs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "x_3vNgyZqz_b",
    "outputId": "092d2277-10ae-490b-bf0f-d53925fa8d32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_city</th>\n",
       "      <th>business_state</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_city_state</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7kfJk_NtOslC9jPk2Koz3g</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>ON</td>\n",
       "      <td>8AW0koYMDa1PlJMOE-b2-g</td>\n",
       "      <td>AjaxON</td>\n",
       "      <td>3.028969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                 user_id  ... business_city_state predictions\n",
       "0      0  7kfJk_NtOslC9jPk2Koz3g  ...              AjaxON    3.028969\n",
       "\n",
       "[1 rows x 7 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VINNqvNPrDMZ"
   },
   "outputs": [],
   "source": [
    "analysis_df = predict_df.merge(top_10_recs, left_on = ['user_id','business_city','index'], right_on = ['user_id','business_city','level_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lI8JXFnqrDjc",
    "outputId": "a5dcb26b-c000-4761-eed3-a39f60143192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(analysis_df.groupby('business_city')['business_id'].count() ==50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BzHo5qVz33g"
   },
   "source": [
    "#### Coverage(% of unique recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_zvl2iWirD0Q",
    "outputId": "bf129774-0c4e-4e1d-88e1-55b4349c970e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23156626506024094"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(analysis_df.groupby('business_city')['business_id'].nunique()/50).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwJxLq7frD37"
   },
   "outputs": [],
   "source": [
    "predict_df['rankings']=predict_df.groupby(['business_city','user_id'])['predictions'].rank(\"first\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqBgeEyJrDyS"
   },
   "outputs": [],
   "source": [
    "running_rankings =0\n",
    "for row in out.iterrows():\n",
    "    row_values = row[1]\n",
    "    user_recs = predict_df[(predict_df['user_id']==row_values['user_id'])\n",
    "                        &(predict_df['business_city']==row_values['business_city'])\n",
    "                         & (predict_df['business_id']==row_values['business_id'])\n",
    "                          ]\n",
    "    assert len(user_recs)==1\n",
    "    running_rankings += user_recs['rankings'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2LHBqYGz6sh"
   },
   "source": [
    "#### Average Ranking of Last Positive Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g5xr4-MorDwG",
    "outputId": "7ccdb76c-32dc-4135-defc-146be2190035"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449.69397590361444"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_rankings / len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Result Comparison\n",
    "\n",
    "Please refer to the report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-323e9717061a>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-323e9717061a>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    **Caveat**: Even for a subsample (ratings_train is a subsample of full ratings dataset), there are restaurants rated by users but not **do not exist** in business profile dataset. This needs to be handled when building recommender system.\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Select restaurants in training set\n",
    "print(business.shape)\n",
    "train_business_id = np.unique(ratings_train.business_id)\n",
    "business = business.loc[business.business_id.isin(train_business_id), :]\n",
    "print(business.shape)\n",
    "\n",
    "### Business Profile\n",
    "\n",
    "tmp_series = pd.Series(business.category.str.split(', ').tolist(), index=business.business_id)\n",
    "business_df = pd.get_dummies(tmp_series.apply(pd.Series).stack()).sum(level=0).reset_index()\n",
    "\n",
    "business_df.head()\n",
    "\n",
    "business_df_normalized = business_df.set_index('business_id') \\\n",
    "                            .div(business_df.set_index('business_id').sum(axis=1)**0.5, axis=0)\n",
    "\n",
    "business_df_normalized.sort_values('business_id', inplace=True)\n",
    "\n",
    "business_df_normalized.head()\n",
    "\n",
    "### User Profile\n",
    "\n",
    "# check if all restaurants that users have rated exist in restuarant profile\n",
    "print('The number of all rated restaurants: {}'.format(len(np.unique(ratings_train[['business_id']]))))\n",
    "print('The number of resturants existing profile: {}'.format(len(np.unique(business_df.business_id))))\n",
    "\n",
    "# **Caveat**: Even for a subsample (ratings_train is a subsample of full ratings dataset), there are restaurants rated by users but not **do not exist** in business profile dataset. This needs to be handled when building recommender system. \n",
    "\n",
    "# Build uer profile\n",
    "users_id = np.unique(ratings_train.user_id)\n",
    "users_df = pd.DataFrame(columns = business_df_normalized.columns)\n",
    "\n",
    "approach_2_start = time.time()\n",
    "# create a for loop to create profile vector for each user\n",
    "for i in tqdm(range(len(users_id))):\n",
    "    selected_user_id = users_id[i]\n",
    "    # select business that user has rated\n",
    "    selected_rating = ratings_train.loc[ratings_train.user_id == selected_user_id, ['business_id', 'rating']]. \\\n",
    "                                        set_index('business_id')\n",
    "    ind = selected_rating.index\n",
    "    # To calculate user profile vector, \n",
    "    # multiply weight (normalized frequency) by ratings across all rated business and take mean of the result\n",
    "    try: \n",
    "        # only need to extract rated restaurants because the rest of term will be \n",
    "        w = business_df_normalized.loc[ind, :]\n",
    "    except KeyError:\n",
    "        business_id_existed_profile = business_df.business_id\n",
    "        filtered_ind = pd.Index([j for j in ind if j in business_id_existed_profile.tolist()])    \n",
    "        w = business_df_normalized.loc[filtered_ind, :]\n",
    "    except:\n",
    "        print('Some other errors occured during iteration i = {}'.format(i))\n",
    "    if (w.shape[0] == 0):\n",
    "        users_df.loc[selected_user_id] = 0\n",
    "    else:\n",
    "        profile_vec = w.mul(selected_rating.loc[:, 'rating'], axis=0).mean(axis=0)\n",
    "        users_df.loc[selected_user_id] = profile_vec\n",
    "        \n",
    "approach_2_duration = (time.time() - approach_2_start) * 1.0 / 60 \n",
    "print('The approach 2 takes {} minutes to run.'.format(approach_2_duration)) \n",
    "\n",
    "users_df.shape\n",
    "\n",
    "### CHECK\n",
    "# IDF vector  \n",
    "DF = business_df_normalized.sum(axis=0)\n",
    "IDF = (business_df.shape[0]/DF).apply(np.log) #log inverse of DF\n",
    "# The dot product of business profile vector and IDF vector gives\n",
    "# weighted score of each business\n",
    "IDF_business_df_normalized = business_df_normalized.mul(IDF.values)\n",
    "IDF_business_df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
